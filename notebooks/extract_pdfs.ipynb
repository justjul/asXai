{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "import sys\n",
    "\n",
    "#Import config file. Update config.py according to your environment\n",
    "from config import path_to_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import requests, json, re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib3.exceptions import ReadTimeoutError\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "from pdfminer.high_level import extract_text, extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTTextLine, LTTextBox\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_source = 'semanticsscholar' #'arxiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_source='semanticsscholar', years=None, data_types=None):\n",
    "    \"\"\"Fetch dataset metadata and text data for given years and data types.\"\"\"\n",
    "    years = (\n",
    "        [str(year) for year in years] if isinstance(years, list) else\n",
    "        [str(years)] if years else\n",
    "        [year for year in os.listdir(os.path.join(path_to_data, 'metadata')) if year.isdigit()]\n",
    "    )\n",
    "    data_types = data_types or ['metadata', 'text']\n",
    "\n",
    "    load_metadata = 'metadata' in data_types\n",
    "    load_textdata = 'text' in data_types\n",
    "\n",
    "    metadata, textdata = [], []\n",
    "    metadata_years, textdata_years = [], []\n",
    "\n",
    "    for year in years:\n",
    "        if load_metadata:\n",
    "            metadata_path = os.path.join(path_to_data, 'metadata', year, f'{dataset_source}_metadata_{year}.parquet')\n",
    "            if os.path.isfile(metadata_path):\n",
    "                metadata.append(pd.read_parquet(metadata_path, engine=\"pyarrow\"))\n",
    "                metadata_years.append(year)\n",
    "        \n",
    "        if load_textdata:\n",
    "            textdata_path = os.path.join(path_to_data, 'text', year, f'{dataset_source}_text_{year}.parquet')\n",
    "            if os.path.isfile(textdata_path):\n",
    "                textdata.append(pd.read_parquet(textdata_path, engine=\"pyarrow\"))\n",
    "                textdata_years.append(year)\n",
    "    \n",
    "    metadata = pd.concat(metadata, axis=0) if metadata else []\n",
    "    textdata = pd.concat(textdata, axis=0) if textdata else []\n",
    "    \n",
    "    msg_parts = []\n",
    "    if load_metadata:\n",
    "        msg_parts.append(f'metadata loaded for years: {metadata_years}')\n",
    "    if load_textdata:\n",
    "        msg_parts.append(f'text data loaded for years: {textdata_years}')\n",
    "    \n",
    "    if msg_parts:\n",
    "        print(\"; \".join(msg_parts))\n",
    "    \n",
    "    if load_metadata and load_textdata and len(metadata) != len(textdata):\n",
    "        raise ValueError(\"Metadata and text data don't have the same length.\")\n",
    "    \n",
    "    output = (data for data in [metadata, textdata] if len(data) > 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_main_fontsize(pdf_path):\n",
    "    sizes = [\n",
    "        character.size\n",
    "        for page_layout in extract_pages(pdf_path)\n",
    "        for element in page_layout if isinstance(element, LTTextContainer)\n",
    "        for text_line in element if isinstance(text_line, LTTextLine)\n",
    "        for character in text_line if isinstance(character, LTChar)\n",
    "    ]\n",
    "    return stats.mode(sizes, keepdims=True)[0][0] if sizes else 0\n",
    "\n",
    "def extract_pdf_sections(pdf_path, authorlist, paperId, possible_section_headings=None, size_threshold=None):\n",
    "    author_last_names = [name.split()[-1] for name in authorlist.split(',')]\n",
    "    last_section_names = {'acknowledgment', 'acknowledgement', 'acknowlegment', 'reference'}\n",
    "    month_markers = {'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'november', 'december',\n",
    "                     'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'nov', 'dec'}\n",
    "    year_markers = {str(y) for y in range(1900, 2030)}\n",
    "    section_list = possible_section_headings or {\n",
    "        'introduction', 'results', 'discussion', 'conclusions', 'methods', 'materials', 'experimental',\n",
    "        'materials and methods', 'experimental procedure', 'related work', 'i.', 'ii.', 'iii.', 'iv.', 'v.', 'vi.'\n",
    "    }\n",
    "\n",
    "    size_threshold = size_threshold if size_threshold is not None else 0.9\n",
    "\n",
    "    page_width, page_height = 595, 842\n",
    "    nword_abstract_th, nword_sections_th = 30, 30\n",
    "\n",
    "    #Determine the most common font size (mode)\n",
    "    size_mode = pdf_main_fontsize(pdf_path)\n",
    "\n",
    "    text_blocks, tag_blocks, nwords_in_blocks = [], [], []\n",
    "    section_to_find, section_heading, tag = 'AUTHORS', 'UNDEFINED', 'UNDEFINED'\n",
    "    reached_end = False\n",
    "    for p, page_layout in enumerate(extract_pages(pdf_path)):\n",
    "        if reached_end:\n",
    "            break\n",
    "\n",
    "        for element in page_layout:\n",
    "            if not isinstance(element, LTTextBox):\n",
    "                continue\n",
    "\n",
    "            x0, y0, x1, y1 = element.bbox\n",
    "            if not (y0 > 0.05*page_height and y1 < 0.95*page_height and x0 > 0.05*page_width and x1 < 0.95*page_width):\n",
    "                continue\n",
    "\n",
    "            filtered_text, sizes = [], []\n",
    "            for text_line in element:\n",
    "                if isinstance(text_line, LTTextLine):\n",
    "                    line_text = [character.get_text() for character in text_line]\n",
    "                    sizes.extend([character.size for character in text_line if isinstance(character, LTChar)])\n",
    "                    line_str = \"\".join(line_text).strip()\n",
    "                    if not re.fullmatch(r\"\\d+\", line_str): # Keeps lines that are NOT just numbers\n",
    "                        filtered_text.append(line_str)\n",
    "                    filtered_text.append(\"\\n\") # Preserve line breaks\n",
    "\n",
    "            #joining characters in a single text while removing weird markers generated by pdfminer\n",
    "            filtered_text = re.sub(r'\\(cid:[^\\)]+\\)', '', \"\".join(filtered_text).strip())\n",
    "            word_list = re.split(r'[\\n\\s]+', filtered_text.lower().strip())\n",
    "            nwords = len(word_list)\n",
    "\n",
    "            if any(end_section in ' '.join(word_list[:3]) for end_section in last_section_names):\n",
    "                reached_end = True\n",
    "                continue\n",
    "            \n",
    "            #removing everything before the author block as well as the correspondance fields\n",
    "            if p <= 1:\n",
    "                nauthors_detected = sum(lastname.lower() in ' '.join(word_list) for lastname in author_last_names)\n",
    "                if nauthors_detected >= 0.5 * len(author_last_names) and y0 > 0.3 * page_height:\n",
    "                    text_blocks, tag_blocks, nwords_in_blocks = [], [], []\n",
    "                    section_to_find = 'ABSTRACT'\n",
    "                    filtered_text = []\n",
    "                if nauthors_detected > 0 and '@' in filtered_text:\n",
    "                    filtered_text = []\n",
    "\n",
    "            #removing blocks likely headers with publication date\n",
    "            if any([m in word_list for m in month_markers]) and any([y in word_list for y in year_markers]) and nwords < 10:\n",
    "                continue\n",
    "\n",
    "            #removing figure captions\n",
    "            if any([figname in word_list[0] for figname in ['fig', 'figure', 'table', 'image']]):\n",
    "                continue\n",
    "\n",
    "            #removing previous block if likely a header but not followed by capitalized paragraph\n",
    "            if filtered_text and not filtered_text[0].isupper() and nwords_in_blocks and nwords_in_blocks[-1] <= 3:\n",
    "                text_blocks.pop()\n",
    "                tag_blocks.pop()\n",
    "                nwords_in_blocks.pop()\n",
    "            elif (filtered_text and filtered_text.strip() and filtered_text.strip()[0].isupper() and nwords_in_blocks and \n",
    "                  nwords_in_blocks[-1] <= 3 and text_blocks and any(h in re.sub(r'[\\n\\s]+', ' ', text_blocks[-1].lower()) for h in section_list)):\n",
    "                section_heading = ''.join([w.upper() for w in re.sub(r'[\\d.]', '', text_blocks[-1])])\n",
    "                if nwords > nword_sections_th:\n",
    "                    tag_blocks[-1] = section_heading\n",
    "\n",
    "            if not reached_end and filtered_text and (max(sizes, default=0) >= size_threshold * size_mode or nwords > 50):\n",
    "                if section_to_find == 'ABSTRACT' and nwords > nword_abstract_th:\n",
    "                    tag = 'ABSTRACT'\n",
    "                    if word_list[-1][-1] == '.':\n",
    "                        section_to_find = 'INTRODUCTION'\n",
    "                elif section_to_find == 'INTRODUCTION':\n",
    "                    if nwords > nword_sections_th:\n",
    "                        tag = 'INTRODUCTION'\n",
    "                        section_heading, section_to_find = 'INTRODUCTION', 'NEXTHEADING'\n",
    "                    else:\n",
    "                        tag = 'UNDEFINED'\n",
    "                elif section_to_find == 'NEXTHEADING' and nwords > nword_sections_th:\n",
    "                    tag = section_heading\n",
    "                \n",
    "                text_blocks.append(filtered_text)\n",
    "                tag_blocks.append(tag)\n",
    "                nwords_in_blocks.append(nwords)\n",
    "\n",
    "    sections = {'paperId': paperId,\n",
    "            'pdf_abstract': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'abstract'})),\n",
    "            'pdf_introduction': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'introduction', 'related work', 'i.', 'ii.'})),\n",
    "            'pdf_results': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'results', 'experiment', 'i.', 'ii.'})),\n",
    "            'pdf_discussion': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'discussion', 'conclusion', 'v.', 'vi.'})),\n",
    "            'pdf_methods': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'methods', 'materials', 'experimental', 'materials and methods', 'experimental procedure'})),\n",
    "            'pdf_text': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if not any(s in t.lower() for s in {'undefined'})),\n",
    "            'author_list': authorlist\n",
    "            }\n",
    "    \n",
    "    return sections, text_blocks, tag_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs(inputs):\n",
    "    #you'll need to have ChromeDriver installed\n",
    "    txtdata, downloads_dir = inputs\n",
    "    service = Service(executable_path=\"/usr/bin/chromedriver\")\n",
    "    downloads_dir = downloads_dir or './data/pdf/downloads'\n",
    "    os.makedirs(downloads_dir, exist_ok=True)\n",
    "    output_dir = os.path.dirname(downloads_dir)\n",
    "\n",
    "    txtdata, downloads_dir = inputs\n",
    "    service = Service(executable_path=\"/usr/bin/chromedriver\")\n",
    "    if downloads_dir is None:\n",
    "        downloads_dir = './data/pdf/downloads'\n",
    "\n",
    "    output_dir = '/'.join(downloads_dir.split('/')[:-1])\n",
    "    \n",
    "\n",
    "    timeout_seconds = 10\n",
    "    timeout_startdw_seconds = 5\n",
    "    block_markers = {\"captcha\", \"verify you are human\", \"forbidden\", \"this site can’t be reached\"}\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    prefs = {\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.default_directory\": downloads_dir,\n",
    "        \"download.directory_upgrade\": True\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    text_pdfs = []\n",
    "    fraction_print = 0\n",
    "    processed_list = []\n",
    "\n",
    "    for k, paper in enumerate(txtdata):\n",
    "        try:\n",
    "            custom_download_dir = os.path.join(downloads_dir, paper['paperId'])\n",
    "            os.makedirs(custom_download_dir, exist_ok=True)\n",
    "\n",
    "            # Set Chrome's download directory dynamically\n",
    "            driver.execute_cdp_cmd(\n",
    "                    \"Page.setDownloadBehavior\", \n",
    "                    {\"behavior\": \"allow\",\n",
    "                    \"downloadPath\": custom_download_dir}\n",
    "                    )\n",
    "            \n",
    "            clean_url = re.sub(r'^http://', 'https://', paper['openAccessPdf'])\n",
    "            filename = []\n",
    "            driver.set_page_load_timeout(timeout_seconds)\n",
    "            driver.get(clean_url)\n",
    "            end_time = time.time() + timeout_seconds\n",
    "            end_time_startdw = time.time() + timeout_startdw_seconds\n",
    "            \n",
    "            while time.time() < end_time:\n",
    "                time.sleep(0.1)\n",
    "                filename = [fname for fname in glob.glob(os.path.join(custom_download_dir, '*.pdf')) if not any(id in fname for id in processed_list)]\n",
    "                if filename:\n",
    "                    processed_list = [id for id in processed_list if not any(id in fname for fname in filename)]\n",
    "                    if len(filename) > 1:\n",
    "                        raise ImportError('more than one download to process', filename, processed_list)\n",
    "                    filename = filename[0]\n",
    "                    break\n",
    "                if any(marker.lower() in driver.page_source.lower() for marker in block_markers):\n",
    "                    filename = None\n",
    "                    break\n",
    "                if time.time() > end_time_startdw and not glob.glob(os.path.join(custom_download_dir, '*.crdownload')):\n",
    "                    print(f\"Paper {k}: Download not started after {timeout_startdw_seconds} seconds for {clean_url}.\")\n",
    "                    filename = None\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Paper {k}: Download not finished after {timeout_seconds} seconds for {clean_url}.\")\n",
    "                filename = None\n",
    "        \n",
    "            if filename:\n",
    "                filepath = os.path.join(custom_download_dir, filename)\n",
    "                pdf_data,_,_ = extract_pdf_sections(filepath, paper['authorName'], paper['paperId'])\n",
    "                processed_list.append(paper['paperId'])\n",
    "                pdf_data['problem'] = None\n",
    "                shutil.rmtree(custom_download_dir)\n",
    "                # os.remove(filepath)\n",
    "            else:\n",
    "                pdf_data = {'paperId': txtdata[k]['paperId'], 'author_list': paper['authorName'], 'pdf_text': None, 'pdf_abstract': None, 'pdf_introduction': None, \n",
    "                            'pdf_discussion': None, 'pdf_results': None, 'pdf_methods': None, 'problem': 'captcha'}\n",
    "        except (TimeoutException, WebDriverException):\n",
    "            print(f\"Paper {k}: Error loading {clean_url}\")\n",
    "            pdf_data = {'paperId': txtdata[k]['paperId'], 'author_list': paper['authorName'], 'pdf_text': None, 'pdf_abstract': None, 'pdf_introduction': None, \n",
    "                        'pdf_discussion': None, 'pdf_results': None, 'pdf_methods': None, 'problem': 'broken link'}\n",
    "        \n",
    "        text_pdfs.append(pdf_data)\n",
    "        fraction_done = (k + 1) / len(txtdata) * 100\n",
    "        if fraction_done > fraction_print:\n",
    "            print(f\"{fraction_done:.0f}% of papers processed in {downloads_dir}\")\n",
    "            fraction_print = fraction_print + 20\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    #Extracting pdfs from downloads that took too long to be processed immediately\n",
    "    if os.path.isdir(downloads_dir):\n",
    "        for dirname in os.listdir(downloads_dir):\n",
    "            dir_path = os.path.join(downloads_dir, dirname)\n",
    "            if os.path.isdir(dir_path) and not glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "                shutil.rmtree(dir_path)\n",
    "            elif glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "                filepath = glob.glob(os.path.join(dir_path, '*.pdf'))[0]\n",
    "                paper_data = next((pdf_data for pdf_data in text_pdfs if pdf_data['paperId'] == dirname), None)\n",
    "                if paper_data:\n",
    "                    pdf_data,_,_ = extract_pdf_sections(filepath, paper_data['author_list'], paper_data['paperId'])\n",
    "                    for key in pdf_data.keys():\n",
    "                        paper_data[key] = pdf_data[key]\n",
    "                    paper_data['problem'] = None\n",
    "                \n",
    "        shutil.rmtree(downloads_dir)\n",
    "\n",
    "    return text_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2020\n",
    "end_year = 2025\n",
    "num_processes = 10\n",
    "download_basedir = '/home/jul/DST/recoRAG/data/pdf'\n",
    "\n",
    "for year in range(start_year, end_year+1):\n",
    "    print(f\"Downloading pdfs for year {year}\")\n",
    "    metadata, textdata = load_dataset(dataset_source, years=year)\n",
    "    N = len(textdata)\n",
    "    batch_size = 100 #len(textdata) // num_processes\n",
    "    data4pdf = pd.merge(textdata[['paperId','openAccessPdf']], metadata[['paperId','authorName']], on='paperId', how='left')\n",
    "    textdata_list = [(data4pdf.iloc[k*batch_size:min(N, (k+1)*batch_size)].to_dict(orient='records'), os.path.join(download_basedir, str(k))) for k in range(N // batch_size + 1)]\n",
    "    textdata_list = [batch for batch in textdata_list if len(batch[0]) > 0]\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results = list(pool.imap(download_pdfs, textdata_list))\n",
    "\n",
    "    pdf_texts = pd.DataFrame([pdf for result in results for pdf in result])\n",
    "    textdata_pdf = pd.merge(left=textdata, right=pdf_texts, on='paperId', how='left')\n",
    "\n",
    "    textdata_pdf[textdata_pdf['abstract']=='None']['abstract'] = textdata_pdf[textdata_pdf['abstract']=='None']['pdf_abstract']\n",
    "\n",
    "    filepath = os.path.join(path_to_data, 'text', str(year), f'{dataset_source}_textpdf_{year}.parquet')\n",
    "    textdata_pdf.to_parquet(filepath, engine=\"pyarrow\", compression=\"snappy\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(76016)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(textdata['abstract']=='None').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data, pdf_sections, pdf_tags = extract_pdf_sections(os.path.join(path_to_data, 'pdf', '0', 'PIIS009286740500098X (1).pdf'),'I. Krylova,E. Sablin,Jamie M. R. Moore,R. Xu,G. Waitt,J. A. MacKay,D. Juzumiene,J. Bynum,K. Madauss,V. Montana,L. Lebedeva,M. Suzawa,Jon D. Williams,Shawn P. Williams,R. Guy,J. W. Thornton,R. Fletterick,T. Willson,H. Ingraham', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
