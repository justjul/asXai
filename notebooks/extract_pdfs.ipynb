{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "import sys\n",
    "\n",
    "#Import config file. Update config.py according to your environment\n",
    "from config import path_to_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import requests, json, re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib3.exceptions import ReadTimeoutError\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "from pdfminer.high_level import extract_text, extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTTextLine, LTTextBox\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "import operator\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_data, dataset_source='semanticsscholar', years=None, data_types=None):\n",
    "    \"\"\"Fetch dataset metadata and text data for given years and data types.\"\"\"\n",
    "    years = (\n",
    "        [str(year) for year in years] if isinstance(years, list) else\n",
    "        [str(years)] if years else\n",
    "        [year for year in os.listdir(os.path.join(path_to_data, 'metadata')) if year.isdigit()]\n",
    "    )\n",
    "    data_types = data_types or ['metadata', 'text']\n",
    "    data_types = [data_types] if not isinstance(data_types, list) else data_types\n",
    "\n",
    "    load_metadata = 'metadata' in data_types\n",
    "    load_textdata = 'text' in data_types or 'pdf' in data_types\n",
    "    load_pdfdata = 'pdf' in data_types\n",
    "\n",
    "    metadata, textdata, pdfdata = [], [], []\n",
    "    metadata_years, textdata_years, pdfdata_years = [], [], []\n",
    "\n",
    "    for year in years:\n",
    "        if load_metadata:\n",
    "            metadata_path = os.path.join(path_to_data, 'metadata', year, f'{dataset_source}_metadata_{year}.parquet')\n",
    "            if os.path.isfile(metadata_path):\n",
    "                metadata.append(pd.read_parquet(metadata_path, engine=\"pyarrow\"))\n",
    "                metadata_years.append(year)\n",
    "            else:\n",
    "                print(f\"No metadata for year {year}\")\n",
    "        \n",
    "        if load_textdata:\n",
    "            textdata_path = os.path.join(path_to_data, 'text', year, f'{dataset_source}_text_{year}.parquet')\n",
    "            if os.path.isfile(textdata_path):\n",
    "                textdata.append(pd.read_parquet(textdata_path, engine=\"pyarrow\"))\n",
    "                textdata_years.append(year)\n",
    "            else:\n",
    "                print(f\"No text data for year {year}\")\n",
    "\n",
    "        if load_pdfdata:\n",
    "            pdfdata_path = os.path.join(path_to_data, 'pdf', year, f'{dataset_source}_textpdf_{year}.parquet')\n",
    "            if not os.path.isfile(pdfdata_path):\n",
    "                pdfdata_path = os.path.join(path_to_data, 'text', year, f'{dataset_source}_text_{year}.parquet')\n",
    "                print(f\"No pdf data for year {year}. Will try to load text data instead\")\n",
    "            if os.path.isfile(pdfdata_path):\n",
    "                pdfdata.append(pd.read_parquet(pdfdata_path, engine=\"pyarrow\"))\n",
    "                pdfdata_years.append(year)\n",
    "            else:\n",
    "                print(f\"No pdf or text data for year {year}\")\n",
    "    \n",
    "    metadata = pd.concat(metadata, axis=0, ignore_index=True).reset_index(drop=True) if metadata else []\n",
    "    textdata = pd.concat(textdata, axis=0, ignore_index=True).reset_index(drop=True) if textdata else []\n",
    "    pdfdata = pd.concat(pdfdata, axis=0, ignore_index=True).reset_index(drop=True) if pdfdata else []\n",
    "    \n",
    "    msg_parts = []\n",
    "    if load_metadata:\n",
    "        msg_parts.append(f'metadata loaded for years: {metadata_years}')\n",
    "    if load_textdata:\n",
    "        msg_parts.append(f'text data loaded for years: {textdata_years}')\n",
    "    if load_pdfdata:\n",
    "        msg_parts.append(f'pdf data loaded for years: {pdfdata_years}')\n",
    "    \n",
    "    if msg_parts:\n",
    "        print(\"; \".join(msg_parts))\n",
    "    if load_pdfdata:\n",
    "        textdata = pdfdata.set_index(\"paperId\").combine_first(textdata.set_index(\"paperId\")).reset_index(drop=False)\n",
    "\n",
    "    if load_metadata and load_textdata and len(metadata) != len(textdata):\n",
    "        raise ValueError(\"Metadata and text data don't have the same length.\")\n",
    "    \n",
    "    output = (data for data in [metadata, textdata] if len(data) > 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_main_fontsize(pdf_path):\n",
    "    try:\n",
    "        sizes = [\n",
    "        character.size\n",
    "        for page_layout in extract_pages(pdf_path)\n",
    "        for element in page_layout if isinstance(element, LTTextContainer)\n",
    "        for text_line in element if isinstance(text_line, LTTextLine)\n",
    "        for character in text_line if isinstance(character, LTChar)\n",
    "    ]\n",
    "        size_mode = stats.mode(sizes, keepdims=True)[0][0] if sizes else 0\n",
    "    except Exception:\n",
    "        size_mode = None\n",
    "    return size_mode\n",
    "\n",
    "def extract_pdf_sections(pdf_path, authorlist, paperId, possible_section_headings=None, size_threshold=None):\n",
    "    author_last_names = [name.split()[-1] for name in authorlist.split(',')]\n",
    "    last_section_names = {'acknowledgment', 'acknowledgement', 'acknowlegment', 'reference'}\n",
    "    month_markers = {'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'november', 'december',\n",
    "                     'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'nov', 'dec'}\n",
    "    year_markers = {str(y) for y in range(1900, 2030)}\n",
    "    section_list = possible_section_headings or {\n",
    "        'introduction', 'results', 'discussion', 'conclusions', 'methods', 'materials', 'experimental',\n",
    "        'materials and methods', 'experimental procedure', 'related work', 'i.', 'ii.', 'iii.', 'iv.', 'v.', 'vi.'\n",
    "    }\n",
    "\n",
    "    size_threshold = size_threshold if size_threshold is not None else 0.9\n",
    "\n",
    "    page_width, page_height = 595, 842\n",
    "    nword_abstract_th, nword_sections_th = 30, 30\n",
    "\n",
    "    #Determine the most common font size (mode)\n",
    "    size_mode = pdf_main_fontsize(pdf_path)\n",
    "\n",
    "    text_blocks, tag_blocks, nwords_in_blocks = [], [], []\n",
    "    section_to_find, section_heading, tag = 'AUTHORS', 'UNDEFINED', 'UNDEFINED'\n",
    "    reached_end = False\n",
    "    if size_mode is not None:\n",
    "        for p, page_layout in enumerate(extract_pages(pdf_path)):\n",
    "            if reached_end:\n",
    "                break\n",
    "\n",
    "            for element in page_layout:\n",
    "                if not isinstance(element, LTTextBox):\n",
    "                    continue\n",
    "\n",
    "                x0, y0, x1, y1 = element.bbox\n",
    "                if not (y0 > 0.05*page_height and y1 < 0.95*page_height and x0 > 0.05*page_width and x1 < 0.95*page_width):\n",
    "                    continue\n",
    "\n",
    "                filtered_text, sizes = [], []\n",
    "                for text_line in element:\n",
    "                    if isinstance(text_line, LTTextLine):\n",
    "                        line_text = [character.get_text() for character in text_line]\n",
    "                        sizes.extend([character.size for character in text_line if isinstance(character, LTChar)])\n",
    "                        line_str = \"\".join(line_text).strip()\n",
    "                        if not re.fullmatch(r\"\\d+\", line_str): # Keeps lines that are NOT just numbers\n",
    "                            filtered_text.append(line_str)\n",
    "                        filtered_text.append(\"\\n\") # Preserve line breaks\n",
    "\n",
    "                #joining characters in a single text while removing weird markers generated by pdfminer\n",
    "                filtered_text = re.sub(r'\\(cid:[^\\)]+\\)', '', \"\".join(filtered_text).strip())\n",
    "                word_list = re.split(r'[\\n\\s]+', filtered_text.lower().strip())\n",
    "                nwords = len(word_list)\n",
    "\n",
    "                if any(end_section in ' '.join(word_list[:3]) for end_section in last_section_names):\n",
    "                    reached_end = True\n",
    "                    continue\n",
    "                \n",
    "                #removing everything before the author block as well as the correspondance fields\n",
    "                if p <= 1:\n",
    "                    nauthors_detected = sum(lastname.lower() in ' '.join(word_list) for lastname in author_last_names)\n",
    "                    if nauthors_detected >= 0.5 * len(author_last_names) and y0 > 0.3 * page_height:\n",
    "                        text_blocks, tag_blocks, nwords_in_blocks = [], [], []\n",
    "                        section_to_find = 'ABSTRACT'\n",
    "                        filtered_text = []\n",
    "                    if nauthors_detected > 0 and '@' in filtered_text:\n",
    "                        filtered_text = []\n",
    "\n",
    "                #removing blocks likely headers with publication date\n",
    "                if any([m in word_list for m in month_markers]) and any([y in word_list for y in year_markers]) and nwords < 10:\n",
    "                    continue\n",
    "\n",
    "                #removing figure captions\n",
    "                if any([figname in word_list[0] for figname in ['fig', 'figure', 'table', 'image']]):\n",
    "                    continue\n",
    "\n",
    "                #removing previous block if likely a header but not followed by capitalized paragraph\n",
    "                if filtered_text and not filtered_text[0].isupper() and nwords_in_blocks and nwords_in_blocks[-1] <= 3:\n",
    "                    text_blocks.pop()\n",
    "                    tag_blocks.pop()\n",
    "                    nwords_in_blocks.pop()\n",
    "                elif (filtered_text and filtered_text.strip() and filtered_text.strip()[0].isupper() and nwords_in_blocks and \n",
    "                    nwords_in_blocks[-1] <= 3 and text_blocks and any(h in re.sub(r'[\\n\\s]+', ' ', text_blocks[-1].lower()) for h in section_list)):\n",
    "                    section_heading = ''.join([w.upper() for w in re.sub(r'[\\d.]', '', text_blocks[-1])])\n",
    "                    if nwords > nword_sections_th:\n",
    "                        tag_blocks[-1] = section_heading\n",
    "\n",
    "                if not reached_end and filtered_text and (max(sizes, default=0) >= size_threshold * size_mode or nwords > 50):\n",
    "                    if section_to_find == 'ABSTRACT' and nwords > nword_abstract_th:\n",
    "                        tag = 'ABSTRACT'\n",
    "                        if word_list[-1][-1] == '.':\n",
    "                            section_to_find = 'INTRODUCTION'\n",
    "                    elif section_to_find == 'INTRODUCTION':\n",
    "                        if nwords > nword_sections_th:\n",
    "                            tag = 'INTRODUCTION'\n",
    "                            section_heading, section_to_find = 'INTRODUCTION', 'NEXTHEADING'\n",
    "                        else:\n",
    "                            tag = 'UNDEFINED'\n",
    "                    elif section_to_find == 'NEXTHEADING' and nwords > nword_sections_th:\n",
    "                        tag = section_heading\n",
    "                    \n",
    "                    text_blocks.append(filtered_text)\n",
    "                    tag_blocks.append(tag)\n",
    "                    nwords_in_blocks.append(nwords)\n",
    "    else:\n",
    "        print(f\"PDF {pdf_path} likely corrupted\")\n",
    "\n",
    "    sections = {'paperId': paperId,\n",
    "            'pdf_abstract': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'abstract'})),\n",
    "            'pdf_introduction': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'introduction', 'related work', 'i.', 'ii.'})),\n",
    "            'pdf_results': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'results', 'experiment', 'i.', 'ii.'})),\n",
    "            'pdf_discussion': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'discussion', 'conclusion', 'v.', 'vi.'})),\n",
    "            'pdf_methods': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if any(s in t.lower() for s in {'methods', 'materials', 'experimental', 'materials and methods', 'experimental procedure'})),\n",
    "            'pdf_text': '\\n'.join(text for t, text in zip(tag_blocks, text_blocks) if not any(s in t.lower() for s in {'undefined'})),\n",
    "            'author_list': authorlist\n",
    "            }\n",
    "    \n",
    "    return sections, text_blocks, tag_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdfs(inputs):\n",
    "    #you'll need to have ChromeDriver installed\n",
    "    txtdata, downloads_dir = inputs\n",
    "    service = Service(executable_path=\"/usr/bin/chromedriver\")\n",
    "    os.makedirs(downloads_dir, exist_ok=True)\n",
    "\n",
    "    txtdata, downloads_dir = inputs\n",
    "    service = Service(executable_path=\"/usr/bin/chromedriver\")\n",
    "    run_headless = False #True #\n",
    "\n",
    "    timeout_enddw = 10\n",
    "    timeout_startdw = 5\n",
    "    timeout_loadpage = timeout_startdw + timeout_enddw\n",
    "    block_markers = {\"captcha\", \"verify you are human\", \"not a robot\", \"forbidden\", \"this site can’t be reached\"}\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    user_data_dir = tempfile.mkdtemp(prefix=\"chrome-profile-\", dir=\"/tmp\")\n",
    "    options.add_argument(f\"--user-data-dir={user_data_dir}\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    # options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    if run_headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    prefs = {\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.default_directory\": downloads_dir,\n",
    "        \"download.directory_upgrade\": True\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    waitforSession = 0\n",
    "    max_retries = 10\n",
    "    retry_delay = 3\n",
    "    while waitforSession < max_retries:\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "            if run_headless:\n",
    "                driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Session creation failed. Retrying ({waitforSession+1}/{max_retries})...\")\n",
    "            time.sleep(retry_delay)  # Wait before retrying\n",
    "            waitforSession += 1\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to create a Selenium session after multiple attempts. Error: {e}\")\n",
    "\n",
    "    text_pdfs = []\n",
    "    fraction_print = 0\n",
    "    processed_list = []\n",
    "\n",
    "    for k, paper in enumerate(txtdata):\n",
    "        try:\n",
    "            paper_download_dir = os.path.join(downloads_dir, paper['paperId'])\n",
    "            os.makedirs(paper_download_dir, exist_ok=True)\n",
    "\n",
    "            # Set Chrome's download directory dynamically\n",
    "            driver.execute_cdp_cmd(\n",
    "                    \"Page.setDownloadBehavior\", \n",
    "                    {\"behavior\": \"allow\",\n",
    "                    \"downloadPath\": paper_download_dir}\n",
    "                    )\n",
    "            \n",
    "            clean_url = re.sub(r'^http://', 'https://', paper['openAccessPdf'])\n",
    "            filename = []\n",
    "            driver.set_page_load_timeout(timeout_loadpage)\n",
    "            driver.get(clean_url)\n",
    "            end_time_enddw = time.time() + timeout_enddw\n",
    "            end_time_startdw = time.time() + timeout_startdw\n",
    "            \n",
    "            while time.time() < end_time_enddw:\n",
    "                time.sleep(0.1)\n",
    "                filename = [fname for fname in glob.glob(os.path.join(paper_download_dir, '*.pdf')) if not any(id in fname for id in processed_list)]\n",
    "                if filename:\n",
    "                    processed_list = [id for id in processed_list if not any(id in fname for fname in filename)]\n",
    "                    if len(filename) > 1:\n",
    "                        raise ImportError('more than one download to process', filename, processed_list)\n",
    "                    filename = filename[0]\n",
    "                    break\n",
    "                if any(marker.lower() in driver.page_source.lower() for marker in block_markers):\n",
    "                    filename = None\n",
    "                    break\n",
    "                if time.time() > end_time_startdw and not glob.glob(os.path.join(paper_download_dir, '*.crdownload')):\n",
    "                    filename = None\n",
    "                    break\n",
    "            else:\n",
    "                filename = None\n",
    "            \n",
    "        \n",
    "            if filename:\n",
    "                filepath = os.path.join(paper_download_dir, filename)\n",
    "                pdf_data,_,_ = extract_pdf_sections(filepath, paper['authorName'], paper['paperId'])\n",
    "                processed_list.append(paper['paperId'])\n",
    "                pdf_data['openAccessPdf'] = paper['openAccessPdf']\n",
    "                pdf_data['problem'] = None\n",
    "                shutil.rmtree(paper_download_dir)\n",
    "                # os.remove(filepath)\n",
    "            else:\n",
    "                pdf_data = {'paperId': txtdata[k]['paperId'], 'author_list': paper['authorName'], 'pdf_text': None, 'pdf_abstract': None, 'pdf_introduction': None, \n",
    "                            'pdf_discussion': None, 'pdf_results': None, 'pdf_methods': None, 'problem': 'captcha', 'openAccessPdf': paper['openAccessPdf']}\n",
    "        except Exception:\n",
    "            pdf_data = {'paperId': txtdata[k]['paperId'], 'author_list': paper['authorName'], 'pdf_text': None, 'pdf_abstract': None, 'pdf_introduction': None, \n",
    "                        'pdf_discussion': None, 'pdf_results': None, 'pdf_methods': None, 'problem': 'broken link', 'openAccessPdf': paper['openAccessPdf']}\n",
    "        \n",
    "        text_pdfs.append(pdf_data)\n",
    "        fraction_done = (k + 1) / len(txtdata) * 100\n",
    "        if fraction_done > fraction_print:\n",
    "            fraction_print = fraction_print + 20\n",
    "\n",
    "    driver.quit()\n",
    "    try:\n",
    "        os.rmdir(user_data_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #Extracting pdfs from downloads that took too long to be processed immediately\n",
    "    if os.path.isdir(downloads_dir):\n",
    "        for dirname in os.listdir(downloads_dir):\n",
    "            dir_path = os.path.join(downloads_dir, dirname)\n",
    "            if os.path.isdir(dir_path) and not glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "                shutil.rmtree(dir_path)\n",
    "            elif glob.glob(os.path.join(dir_path, '*.pdf')):\n",
    "                filepath = glob.glob(os.path.join(dir_path, '*.pdf'))[0]\n",
    "                paper_data = next((pdf_data for pdf_data in text_pdfs if pdf_data['paperId'] == dirname), None)\n",
    "                if paper_data:\n",
    "                    pdf_data,_,_ = extract_pdf_sections(filepath, paper_data['author_list'], paper_data['paperId'])\n",
    "                    for key in pdf_data.keys():\n",
    "                        paper_data[key] = pdf_data[key]\n",
    "                    paper_data['problem'] = None\n",
    "                \n",
    "        shutil.rmtree(downloads_dir)\n",
    "\n",
    "    return text_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_PDFs(path_to_data, dataset_source=None, years=None, n_jobs=None, filters=None):\n",
    "    dataset_source = 'semanticsscholar' if dataset_source is None else dataset_source\n",
    "    years = [datetime.now().year] if years is None else years\n",
    "    years = [years] if not isinstance(years, list) else years\n",
    "\n",
    "    download_basedir = os.path.join(path_to_data, 'temp')\n",
    "    os.makedirs(download_basedir, exist_ok=True)\n",
    "    output_dir = os.path.join(path_to_data, 'pdf')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    #Cleaning-up tmp dir\n",
    "    for dirname in os.listdir(\"/tmp\"):\n",
    "        if \"chrome-profile-\" in dirname:\n",
    "            shutil.rmtree(os.path.join(\"/tmp\", dirname))\n",
    "\n",
    "    # filters = [['abstract','==','None']]\n",
    "    filters = [filters] if filters is not None and not isinstance(filters[0], list) else filters\n",
    "    n_jobs = multiprocessing.cpu_count() if n_jobs is None else n_jobs\n",
    "\n",
    "    ops = {\n",
    "            \"==\": operator.eq,\n",
    "            \"!=\": operator.ne,\n",
    "            \">\": operator.gt,\n",
    "            \"<\": operator.lt,\n",
    "            \">=\": operator.ge,\n",
    "            \"<=\": operator.le,\n",
    "            \"isna\": pd.isna,\n",
    "            \"notna\": pd.notna\n",
    "        }\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Downloading pdfs for year {year}\")\n",
    "        metadata, textdata = load_dataset(path_to_data, dataset_source, years=year, data_types=['metadata', 'pdf'])\n",
    "        filtered_indices = textdata.index\n",
    "        if filters:\n",
    "            for col, op, val in filters:\n",
    "                if op in [\"isna\", \"notna\"]:\n",
    "                    idx_list = textdata[ops[op](textdata[col])].index\n",
    "                else:\n",
    "                    idx_list = textdata[ops[op](textdata[col], val)].index\n",
    "\n",
    "                filtered_indices = filtered_indices.intersection(idx_list)\n",
    "\n",
    "        data4pdf = pd.merge(textdata[['paperId','openAccessPdf']].loc[filtered_indices], metadata[['paperId','authorName']].loc[filtered_indices], on='paperId', how='inner')\n",
    "        if len(data4pdf) < len(filtered_indices):\n",
    "            raise ValueError(\"List of metadata and text data are notmatching the same articles\")\n",
    "        \n",
    "        minibatch_size = 50\n",
    "        batch_size = 1 * n_jobs * minibatch_size\n",
    "        Npapers = batch_size #*5 #len(data4pdf)\n",
    "        with tqdm(range(Npapers // batch_size + 1), desc=f'Year {year} / {len(years)} years ({Npapers} papers on that year)', unit='batch') as pbar:\n",
    "            try:\n",
    "                for i in pbar:\n",
    "                    batch = data4pdf.iloc[i*batch_size:min(Npapers, (i+1)*batch_size)].to_dict(orient='records')\n",
    "                    Nbatch = len(batch)\n",
    "                    textdata_list = [(data4pdf.iloc[k*minibatch_size:min(Nbatch, (k+1)*minibatch_size)].to_dict(orient='records'), os.path.join(download_basedir, str(k))) for k in range(Nbatch // minibatch_size + 1)]\n",
    "                    textdata_list = [batch for batch in textdata_list if len(batch[0]) > 0]\n",
    "                    if textdata_list:\n",
    "                        with multiprocessing.Pool(processes=n_jobs) as pool:\n",
    "                            results = list(pool.imap(extract_pdfs, textdata_list))\n",
    "\n",
    "                        pdfdata = pd.DataFrame([pdf for result in results for pdf in result])\n",
    "                        textdata = pdfdata.set_index(\"paperId\").combine_first(textdata.set_index(\"paperId\")).reset_index(drop=False)\n",
    "\n",
    "                        mask = textdata['abstract'] == 'None'\n",
    "                        textdata.loc[mask, 'abstract'] = textdata.loc[mask, 'pdf_abstract']\n",
    "            except Exception as e:\n",
    "                pbar.close()\n",
    "                raise\n",
    "\n",
    "\n",
    "        output_dir_year = os.path.join(output_dir, str(year))\n",
    "        os.makedirs(output_dir_year, exist_ok=True)\n",
    "        filepath = os.path.join(output_dir_year, f'{dataset_source}_textpdf_{year}.parquet')\n",
    "\n",
    "        textdata.to_parquet(filepath, engine=\"pyarrow\", compression=\"snappy\", index=True)\n",
    "\n",
    "    shutil.rmtree(download_basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pdfs for year 2005\n",
      "No pdf data for year 2005. Will try to load text data instead\n",
      "metadata loaded for years: ['2005']; text data loaded for years: ['2005']; pdf data loaded for years: ['2005']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Year 2005 / 1 years (1000 papers on that year):   0%|          | 0/2 [00:00<?, ?batch/s]The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/2/005ae77005a4138ba8a27cd4c667a237dd268230/yigak03062.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/2/005ae77005a4138ba8a27cd4c667a237dd268230/yigak03062.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/3/00839b4b2347faad03b41eaadb3191b8c81efada/article.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/3/00839b4b2347faad03b41eaadb3191b8c81efada/article.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/0/000fd65de92d766ada2dab424f7f66e526e2f063/Cavallaro2005_1275.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/0/000fd65de92d766ada2dab424f7f66e526e2f063/Cavallaro2005_1275.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/11/01ff3e30074e61c4b456be3be166e63dd78465a7/000498932.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/11/01ff3e30074e61c4b456be3be166e63dd78465a7/000498932.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/13/0269f75280b5d5429934c20f617ce556d0dbc8ea/58_JJID.2005.195.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/13/0269f75280b5d5429934c20f617ce556d0dbc8ea/58_JJID.2005.195.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/17/030cf4596df70494483c059c1d1d78c274dcce1e/58_JJID.2005.31.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/17/030cf4596df70494483c059c1d1d78c274dcce1e/58_JJID.2005.31.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/0/002b2d9da5439eb1d847ab261f03e536e0880362/54_461.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/0/002b2d9da5439eb1d847ab261f03e536e0880362/54_461.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/2/0055f213e8a8a2a7f4dec08cf90900a3853ad849/2_465.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/home/jul/DST/recoRAG/data/temp/2/0055f213e8a8a2a7f4dec08cf90900a3853ad849/2_465.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "Year 2005 / 1 years (1000 papers on that year): 100%|██████████| 2/2 [05:30<00:00, 165.21s/batch]\n"
     ]
    }
   ],
   "source": [
    "download_PDFs(path_to_data, years=2005, n_jobs=20)#, filters=[['abstract','==','None']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata loaded for years: ['2005']; text data loaded for years: ['2005']; pdf data loaded for years: ['2005']\n"
     ]
    }
   ],
   "source": [
    "metadata, textdata = load_dataset(path_to_data, years=[2005], data_types=['metadata', 'pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(447)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdata.iloc[:1000][\"pdf_text\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age     name  taille\n",
      "0   1   25    Alice     NaN\n",
      "1   2   30      Bob     NaN\n",
      "2   3   36  Charlie   170.0\n",
      "3   4   40    David   180.0\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35],\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    \"id\": [3, 4],  # id=2 and id=3 exist, id=4 is new\n",
    "    \"name\": [\"Charlie\", \"David\"],  # id=3 has new data\n",
    "    \"age\": [36, 40],  # Updates for id=2 and id=3\n",
    "    \"taille\": [170, 180]\n",
    "})\n",
    "\n",
    "# 🔥 Merge and update existing rows\n",
    "merged_df = df2.set_index(\"id\").combine_first(df1.set_index(\"id\")).reset_index(drop=False)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data, pdf_sections, pdf_tags = extract_pdf_sections(os.path.join(path_to_data, 'pdf', '0', 'PIIS009286740500098X (1).pdf'),'I. Krylova,E. Sablin,Jamie M. R. Moore,R. Xu,G. Waitt,J. A. MacKay,D. Juzumiene,J. Bynum,K. Madauss,V. Montana,L. Lebedeva,M. Suzawa,Jon D. Williams,Shawn P. Williams,R. Guy,J. W. Thornton,R. Fletterick,T. Willson,H. Ingraham', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
