{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#Import config file. Update config.py according to your environment\n",
    "from config import path_to_data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import requests, json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from adapters import AutoAdapterModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_source = 'semanticsscholar' #'arxiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_source='semanticsscholar', years=None, data_types=None):\n",
    "    \"\"\"Fetch dataset metadata and text data for given years and data types.\"\"\"\n",
    "    years = (\n",
    "        [str(year) for year in years] if isinstance(years, list) else\n",
    "        [str(years)] if years else\n",
    "        [year for year in os.listdir(os.path.join(path_to_data, 'metadata')) if year.isdigit()]\n",
    "    )\n",
    "    data_types = data_types or ['metadata', 'text']\n",
    "\n",
    "    load_metadata = 'metadata' in data_types\n",
    "    load_textdata = 'text' in data_types\n",
    "\n",
    "    metadata, textdata = [], []\n",
    "    metadata_years, textdata_years = [], []\n",
    "\n",
    "    for year in years:\n",
    "        if load_metadata:\n",
    "            metadata_path = os.path.join(path_to_data, 'metadata', year, f'{dataset_source}_metadata_{year}.parquet')\n",
    "            if os.path.isfile(metadata_path):\n",
    "                metadata.append(pd.read_parquet(metadata_path, engine=\"pyarrow\"))\n",
    "                metadata_years.append(year)\n",
    "        \n",
    "        if load_textdata:\n",
    "            textdata_path = os.path.join(path_to_data, 'text', year, f'{dataset_source}_text_{year}.parquet')\n",
    "            if os.path.isfile(textdata_path):\n",
    "                textdata.append(pd.read_parquet(textdata_path, engine=\"pyarrow\"))\n",
    "                textdata_years.append(year)\n",
    "    \n",
    "    metadata = pd.concat(metadata, axis=0) if metadata else []\n",
    "    textdata = pd.concat(textdata, axis=0) if textdata else []\n",
    "    \n",
    "    msg_parts = []\n",
    "    if load_metadata:\n",
    "        msg_parts.append(f'metadata loaded for years: {metadata_years}')\n",
    "    if load_textdata:\n",
    "        msg_parts.append(f'text data loaded for years: {textdata_years}')\n",
    "    \n",
    "    if msg_parts:\n",
    "        print(\"; \".join(msg_parts))\n",
    "    \n",
    "    if load_metadata and load_textdata and len(metadata) != len(textdata):\n",
    "        raise ValueError(\"Metadata and text data don't have the same length.\")\n",
    "    \n",
    "    output = (data for data in [metadata, textdata] if len(data) > 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2005\n",
    "\n",
    "metadata, textdata = fetch_dataset(dataset_source, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'allenai/specter2'#'sentence-transformers/all-mpnet-base-v1' #\n",
    "batch_size = 128\n",
    "max_length = 512\n",
    "Nsamples = len(data)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sentence-transformer' in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "elif 'specter2' in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "    model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "    model.load_adapter(model_name, source=\"hf\", set_active=True)\n",
    "\n",
    "model.to(device);\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input\n",
    "embeddings = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(range(Nsamples//batch_size + 1), desc=f'total of {Nsamples} papers', unit='batch') as pbar:\n",
    "        for i in pbar:\n",
    "            batch_text = data.iloc[i*batch_size:(i + 1)*batch_size]['title'] + tokenizer.sep_token + data.iloc[i*batch_size:(i + 1)*batch_size]['abstract']\n",
    "            batch_text = batch_text.to_list()\n",
    "            if batch_text:\n",
    "                batch_tokens = tokenizer(batch_text, padding=True, truncation=True, return_tensors=\"pt\", return_token_type_ids=False, max_length=max_length)\n",
    "                for key in batch_tokens.keys():\n",
    "                    batch_tokens[key] = batch_tokens[key].to(device)\n",
    "                \n",
    "                output = model(**batch_tokens)\n",
    "                # first token in the batch as the embedding\n",
    "                if 'pooler_output' in output.keys():\n",
    "                    embeddings_batch = output.pooler_output.cpu().numpy().astype(np.float32)\n",
    "                else:\n",
    "                    embeddings_batch = output.last_hidden_state[:, 0, :].cpu().numpy().astype(np.float32)\n",
    "                    \n",
    "                embeddings.append(embeddings_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nickname = model_name.split('/')[-1]\n",
    "embeddings_filepath = os.path.join(config.path_to_data, 'embeddings', f'embeddings_{dataset_source}_{model_nickname}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.concat(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should save as hf5 instead of HDFStore\n",
    "B, D = embeddings.shape\n",
    "with pd.HDFStore(embeddings_filepath, mode=\"w\", complib=\"blosc\", complevel=9) as store:\n",
    "    store.put(\"metadata\", data, format=\"table\", data_columns=True)\n",
    "    store.put(\"embeddings\", pd.DataFrame(embeddings).set_index(data.index), format=\"table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
