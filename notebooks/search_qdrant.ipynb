{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c261e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import config\n",
    "from dataIO import load_data\n",
    "from vectorDB import QdrantManager\n",
    "from vectorDB import PaperEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86234c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-04-11 18:22:42,699 - vectorDB.push_qdrant - INFO - Qdrant container already up and running\n"
     ]
    }
   ],
   "source": [
    "embedEngine = PaperEmbed(model_name=config.MODEL_EMBED)\n",
    "qdrant = QdrantManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829e580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what's the role of the basolateral amygdala?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057ec5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = embedEngine.embed_queries([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b57501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "res = await qdrant.query_batch_streamed(query_vectors=query_emb.tolist())\n",
    "match_titles = [res[0].groups[k].lookup.payload['title'] for k in range(len(res[0].groups))]\n",
    "match_texts = [res[0].groups[k].lookup.payload['main_text'] for k in range(len(res[0].groups))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66941057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37617e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# model_id = \"unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit\"\n",
    "model_id = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058474e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = '\\n\\nabstract#'.join(f\"{k}\\n\" + txt for k, txt in enumerate(match_texts[2:3]))\n",
    "query=\"What is the role of amygdala?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "Summarize the article below to help answer a question. \n",
    "Do not directly answer the question, instead summarize the evidence present in the excerpt that\n",
    "help answer the question. \n",
    "Reply ’Not applicable’ if the excerpt is irrelevant. \n",
    "At the end of your response, provide a score from 1-10 on a newline indicating the relevance\n",
    "of the text to the question. \n",
    "DO NOT EXPLAIN YOUR SCORE.\n",
    "     \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Question: {query}\n",
    "Article: \n",
    "{articles}\n",
    "\n",
    "Relevant Information Summary:\"\"\"}\n",
    "]\n",
    "# query=\"What is the role of amygdala?\"\n",
    "# messages = f\"\"\"<s>[SYSTEM_PROMPT]Summarize the article below to help answer a question. \n",
    "# Do not directly answer the question, instead summarize the evidence present in the excerpt that\n",
    "# help answer the question. \n",
    "# Reply ’Not applicable’ if the excerpt is irrelevant. \n",
    "# At the end of your response, provide a score from 1-10 on a newline indicating the relevance\n",
    "# of the text to the question. \n",
    "# Do not explain your score.[/SYSTEM_PROMPT]\n",
    "\n",
    "# [INST]\n",
    "# Question: {query}\n",
    "# Article: \n",
    "# {articles}\n",
    "\n",
    "# Relevant Information Summary:[/INST]\"\"\"\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=10000,\n",
    ")\n",
    "# print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6bdb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article discusses the role of dynorphin-expressing neurons in the central amygdala (CeADyn) in alcohol drinking. The study used fiber photometry to record real-time activity of these neurons in male and female mice during voluntary alcohol consumption, water consumption, and sucrose consumption. The results showed that there is a unique functional signature for alcohol in the CeADyn neuron population, with a large increase in calcium transients after bouts of licking for alcohol, and only modest increases during licking for water or 0.5% sucrose. This increase in activity was present even when controlling for differences in drinking behavior unique to alcohol, such as longer bout durations. The study suggests that understanding the specific brain mechanisms underlying the behavioral and physiological responses to alcohol that promote alcohol misuse is important for understanding the role of these neurons in alcohol drinking.\n",
      "\n",
      "Relevance Score: 10\n",
      "\n",
      "Explanation: The article provides evidence that the central amygdala, specifically the dynorphin-expressing neurons, play a role in alcohol drinking. The study shows that these neurons are uniquely engaged during alcohol consumption, and this engagement is not fully explained by differences in drinking behavior unique to alcohol. This suggests that these neurons may play a role in promoting alcohol misuse.\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1259c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses a study that investigates the activity of dynorphin-expressing neurons in the central amygdala (CeADyn) during alcohol drinking in mice. Key findings include:\n",
      "\n",
      "- CeADyn neurons show a large increase in calcium transients after bouts of licking for alcohol, compared to modest increases for water or sucrose.\n",
      "- The unique engagement of CeADyn neurons during alcohol consumption is not fully explained by differences in drinking behavior, such as longer bout durations for alcohol.\n",
      "- The study used fiber photometry to record neuronal activity and multilevel modeling to analyze the data, revealing significant effects of time and solution type on neuronal activity.\n",
      "- The study also explored the relationship between drinking characteristics (such as bout duration and temporal occurrence) and neuronal activity, finding that while these characteristics modulate signal, they do not fully explain the unique alcohol-evoked CeADyn activity.\n",
      "- The study suggests that the unique physiological action of alcohol on CeADyn neurons may be due to chemesthetic sensations or the distinct taste characteristics of alcohol.\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][\"generated_text\"].split(\"[/INST]\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18fb84a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[SYSTEM_PROMPT]You are a research assistant. Answer the user’s query as accurately as possible using your knowledge and the provided articles.     Do not summarize the abstracts unless explicitly asked. Use your knowledge first if the question is general. Try as much as possible to cite the provided abstract which are      referenced as abstract#1, abstract#2, etc[/SYSTEM_PROMPT]     [INST]Here are the abstracts of recent papers:\n",
      "\n",
      "0\n",
      "AmygdalaGo-BOLT3D: A boundary learning transformer for tracing human amygdala 1Center for Brain Imaging Science and Technology, Zhejiang University, Hangzhou, 310027, Zhejiang, China. 2College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, 310027, China. 3State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, 100875, China. 4Faculty of Psychology, Beijing Normal University, Beijing, 100875, Beijing, China. 5School of Psychology, Capital Normal University, Beijing, 100190, China. 6School of Physics, Zhejiang University, Hangzhou, 310058, Zhejiang, China. 7State Key Laboratory of Brain-Machine Intelligence, Zhejiang University, Hangzhou, 310058, Zhejiang, China. 8Developmental Population Neuroscience Research Center, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China. 9National Basic Science Data Center, Beijing, 100190, China. 10Developmental Population Neuroscience Research Base for Education, Nanning Normal University, Nanning, 530001, China. *Corresponding author(s). E-mail(s): hhezju@zju.edu.cn; xinian.zuo@bnu.edu.cn; †These authors contributed equally to this work. The amygdala is indispensable for mood regulation, cognitive learning, and memory consolidation, and has been closely associated with behavioral pathologies, particularly during critical stages of socio-emotional maturation such as childhood and adoles- cence (Sammallahti et al, 2023; Chai et al, 2023; Par´e and Headley, 2023). Magnetic Resonance Imaging (MRI) of brain tissue structures enables a detailed examination and volumetric analysis of the amygdala, facilitating the construction of developmental neuropsychiatric conditions (Song, 2023; Ben-Zion et al, 2023). Therefore, a volumetric analysis based on a precise segmentation of the amygdala is essential for these studies. Amygdala segmentation has relied on histological atlases and manual segmen- tation techniques utilizing in vivo neuroimaging data. However, while serving as a golden standard, these methods predominantly involve manual labeling, making them labor intensive and prone to errors. Given the necessity to process large datasets, this task becomes particularly cumbersome and requires the development of automatic segmentation methods. A frequently used technique in automatic methods involves probabilistic estimates derived from Markov random fields (Fischl et al, 2002) to assign voxel labels in anatomical MRI images. This approach has been widely implemented in popular software such as FreeSurfer (Fischl et al, 2002), achieving notable segmen- tation performance (Trombini et al, 2023). Other research has used Bayesian methods to delineate brain anatomical boundaries (Patenaude et al, 2011), based on the sig- nal intensity of MRI images and the structural configurations defined by established atlases, such as those used in FSL-FIRST and similar segmentation algorithms (Liu and Chou, 2023). In addition, multi-atlas label fusion segmentation techniques (Xie et al, 2023) integrate information from multiple atlases to reduce mislabeling errors resulting from inaccurate registrations and mitigate the inherent instability of MRI data. This methodology has proven particularly effective in scenarios involving high variability in Markov random fields covering individual disparities and pathological lesions, and has been incorporated into volBrain (Manj´on and Coup´e, 2016) and more recent algorithms (Xie et al, 2023; Liu et al, 2023b). Despite the plethora of available automated brain image segmentation tools, including FreeSurfer and volBrain, it is noteworthy that these toolkits were not developed exclusively for amygdala segmentation. Recent investigations have revealed significant discrepancies in the quantitative estimates of amygdala volume when using existing software algorithms, which can affect the validity of amygdala-centric research findings (Zhou et al, 2021). That is, inaccurate segmentation can introduce systematic biases, compromising the precision and reliability of volumetric measurements. In summary, the accurate segmentation of the human amygdala poses several chal- lenges. Firstly, the amygdala constitutes merely 0.05% of the entire brain, making it susceptible to false positive segmentations. Secondly, although there are distinct class differences between the amygdala and adjacent tissue structures, these differences are minimal, only about 12.6%, compared to surrounding tissues. The complexity of neighboring structures further complicates the segmentation task due to low contrast issues (Pruessner et al, 2000; Fischl et al, 2002; Manj´on and Coup´e, 2016). In this Methods paper, we propose an innovative algorithm to take advantage of manually traced large-scale amygdalae (Zhou et al, 2021) for the transformer model (Lee et al, 2022) that emphasizes boundary delineation to address the challenges mentioned above in automated amygdala segmentation. We delineate the amygdala segmentation into three critical phases: 1) robust global localization, 2) precise delineation and 3) generalizable segmentation. In examining the structure of the amygdala, the observer must first capture the entire cerebral image and subsequently estimate the approximate position of the amygdala. This estimate relies on prior knowledge of the anatomy of the limbic system, where the amygdala is located, and considers adjacent structures such as the hippocampus. Subsequent to this initial phase, the observer leverages a meticulous understanding of the amyg- dala’s internal morphology, gray matter intensities, and other pertinent information to execute a detailed regional delineation. To enhance the segmentation process, a transformer-based algorithm is employed to discern boundary contrasts. This approach aims to improve both the robustness of global amygdala localization and the precision following key elements. 1. To achieve comprehensive detection of amygdala positioning in T1-weighted (T1w) images, we employed a transformer-based network named BOLT3D. The BOLT3D model constructs a multigranularity adaptive collaboration (MGAC) architecture, integrating a global coarse-grained perceptual module to elucidate the global semantics of the amygdala structure and enhance positional sensitivity. Given that intricate details are often neglected in global representations, a local fine-grained enhancement module was developed to augment the perception and retention of detailed information. To harmonize coarse-grained global and fine-grained local data, a multi-granular adaptive fusion module is introduced. This allows the model to concurrently consider the differences between the entire amygdala and adjacent structures, as well as the similarities within the amygdala. This design facilitates the recognition of global positioning and local disparity features, thereby improving the localization accuracy and robustness of amygdala structures. The overarch- ing design objective is to accomplish a more precise and exhaustive detection of amygdala localization contours. 2. To address the challenge of dependency on manual segmentation boundaries and enhance the congruence between predicted and actual delineations, a novel learning strategy for boundary contrasts is introduced. This method enhances the discrim- inability of limbic structures from adjacent tissues by amplifying the characteristic differences between the amygdala and its surrounding tissues, while minimizing the intra-amygdala variations. Consequently, this refines the alignment between automated boundary segmentation and manual segmentation outcomes, thereby enabling a more precise demarcation of the amygdala boundaries. 3. To ensure the generalization efficacy of BOLT3D, a Segment Anything model (SAM) is devised for the refinement of coarse predictions. Specifically, the coarse outputs of the MAGC module are sampled as point cues for the SAM model, uti- lizing the coarse segmentation as a pre-segmentation step for further refinement. Concurrently, the original SAM model is adapted to the amygdala context, compris- ing three essential components: a lightweight volumetric feature encoder, a 3D cue encoder, and a volumetric mask decoder, thereby enhancing the model’s generalized segmentation performance. Initially, we derived global and local semantic features from the original MRI image data employing a multigranularity semantic generation module as depicted in Figure 1(b). Specifically, for T1w image data I, the multigranularity semantic gen- eration module exploits the local induction capability of convolution by employing two 3D convolutions to extract the primary local features Fl. This module facilitates the computation of the interrelationships among feature points, thereby enabling the extraction of global semantic information from regions that exhibit high response in the initial position features. Subsequently, we construct the preliminary global feature Fg. Given a feature map F of dimensions d × H × W , F is derived from the input image I via a single convolutional layer and subsequently projected onto a weight map and a mapping F through two convolutional layers. The channel count on the weight map is hw (h ≤ H, w ≤ W ), where h and w represent the dimensions of the predefined global semantic map. The weight map is then flattened for the Softmax computation, and the resulting weights are used to aggregate the semantic informa- tion within the F mapping by matrix multiplication, resulting in Fg. For succinctness in the illustration, dimensions are presented in abbreviated form. The MGAC module is composed of three components: a global module for coarse- grained perception, a local module for fine-grained enhancement, and an adaptive module for multigranularity fusion, as depicted in Figure 1(c). These components interactively to enhance the model’s performance. Figure 1(b) illustrates the position- ing of the adaptive MGAC module within the overall architecture, while Figure 1(c) Each MGAC module receives two inputs, specifically global coarse-grained seman- tic characteristics Fg and local fine-grained semantic features Fl. To mitigate the tics, BatchNorm3D is used to normalize the inputs. Furthermore, the linear position projection from previous work is mapped according to the positional relationships of the elements, ensuring that the arrangement of tokens within the transformer 2021). However, this approach neglects local structural information, which is vital for image data. Although integrating positional encoding can enable the transformer to extensive training data, which poses a challenge in the realm of medical image anal- layers, thereby incorporating the inductive bias inherent to convolutions. This substi- tution facilitates the preservation of local structural information intrinsic to the image The three-dimensional depth-wise convolutionary technique is employed to con- tant features, denoted Fg and Fl, are subsequently flattened into a one-dimensional sequence to facilitate Transformer processing. Upon this transformation, Fg is con- g and a global content vector F v g . Similarly, Fl, which encapsulates detailed information, is transformed into a local index vector F k l and a local content vector F v l through analogous operations. Regarding the global coarse-grained perception module and the local fine-grained enhancement module, their internal architectures remain consistent, as illustrated in Figure 1b. These modules primarily comprise two components: a multi-head self-attention mechanism, Zl replaces the input and is initially expanded to a one- dimensional sequence. A fully connected layer then serves as the transformation layer, converting the input into a query vector, an index vector, and a content vector. Subse- quently, the multi-head attention matrix is computed by formula 1. The feedforward layer further refines the interrelationships among voxel features, while integration with the residual mechanism augments data flow. This approach proficiently preserves the original features and mitigates gradient vanishing, as detailed in formula 2. l = M HSA (LN (Zl)) + Zl, l′ = F F N (LN (Z ′ In this context, M HSA(·) denotes the multi-head self-attention mechanism, LN (·) signifies layer normalization, and F F N (·) represents the feedforward neural network. The global coarse-grained perception module and the local fine-grained enhancement module employ the computational algorithms delineated in formulas 1 and 2, respec- tively, to transform the global content vector F v g and the local content vector F v inputs. Upon completion of the processing, the resulting updated global content vector g′ and the local content vector F v F v l′ are obtained. Within the context of the adaptive multigranularity fusion module, as illustrated in Figure 1, the correlation matrix Att is derived using F q g as the query vector and F k l as the index vector. The correlation is computed by applying the formula 3, as follows: Atti,j denotes the i-th voxel characteristic within the global query vector F q the corresponding i -th voxel characteristic in the local index vector F k l . The corre- lation score between the characteristics of the j th voxel is computed accordingly. To mitigate overly concentrated correlations in specific local regions and facilitate the model optimization process, a scaling factor of 1/ dk is implemented, where dk signi- fies the dimension of an individual attention head in terms of the number of channels Upon deriving the similarity matrix Att, it is necessary to remap it into the orig- inal value space, comprising both the global content vector space F v g and the local content vector space F v l . Specifically, given the similarity matrix Att, for the local fine-grained branch F v l , Att undergoes direct propagation into a Softmax function to produce a sparse feature distribution. This distribution is then subjected to matrix multiplication with F v l , followed by convolution operations to further refine the feature distribution. This operational sequence is encapsulated in Equation 4. In contrast, for the global coarse-grained branch F v g , the matrix Att requires transposition to deter- mine the similarity score between the individual feature points and the query feature. Subsequently, matrix multiplication with F v g is performed, and the resultant prod- uct is processed through deep separable convolutions to enhance the smoothness of the feature distribution. In addition, a residual technique is employed to preserve the integrity of the information and mitigate issues of vanishing gradients. Ultimately, this module yields the outputs F sof tmax(Att)F v l sof tmax(Att⊤)F v g A vast majority of existing 3D segmentation algorithms (C¸ i¸cek et al, 2016; Chen et al, 2019; Jiang et al, 2022) typically neglect explicit segmentation of structural bound- aries. However, certain algorithms strive to improve boundary delineation through attention mechanisms (Liu et al, 2023a; Cheng et al, 2019). Despite these efforts, there remains a paucity of comprehensive and targeted research focused on evaluating the influence of boundary regions on segmentation performance, resulting in subopti- mal overall model performance. To address this gap, we propose to meticulously align boundary information from manual annotations with model segmentation outputs and introduce an innovative boundary contrast learning framework. Previous contrastive learning methodologies have been predominantly applied at the image level. For example, positional (Tang et al, 2022) extends 3D images to 2D sequences and takes advantage of positional relationships within the 2D sequence to determine spatial relationships between positive and negative samples for contrastive learning. Mask Transfiner (Ke et al, 2022) adopts contrastive learning in image patches rather than at the image level, utilizing pseudolabels to derive patches with dis- tinct semantic categories to form positive and negative sample pairs. However, these approaches are inadequate for high-resolution segmentation, since accurate amygdala segmentation requires proximity of same-type pixels throughout the 3D image, separa- tion of different-type pixels, and clustering of pixels based on their labels. Therefore, in this study, the boundary pixels are used to construct positive and negative sample pairs to maximize the distinction between the boundary pixels and adjacent nonrelevant pixels, as illustrated in Figure 1(d). Specifically, we designate the local neighborhood around the boundary as a subset, creating the boundary contrast learning strategy from overlapping 3D patch areas distributed throughout the 3D space. For each sub- set, kernels adjacent to the boundary select a 3D patch space dimension. Manually segmented amygdala data serve as a basis for constructing positive and negative sam- ples. The InfoNCE information maximization algorithm (Radford et al, 2021) is then utilized to calculate the disparity between positive and negative samples in the lati- tude space on the hypersphere. Taking into account the anatomical dimensions of the amygdala, the size of the kernel is set to 5 × 5 × 5, and each subset forms positive and negative samples within this specified area. As illustrated in Figure 1, the formula 6 is specifically designed to focus on the boundary voxels by exclusively computing them. Initially, all boundary regions Bl in manual annotations are considered, and for each voxel xi in Bl, the sampling scope for positive and negative voxels is confined to its local neighborhood Ni. Under this constraint, the positive sample xj ∈ Ni ∧ li = lj for the boundary voxel xi is identified, while the remaining neighboring voxels are treated as negative samples xj ∈ Ni ∧ li ̸= lj. In this paper, j denotes the sample voxel within the neighborhood Ni corresponding to the voxel i, with li and lj representing the categories of differ- ent voxels xi and xj, respectively. Consequently, the boundary voxel contrast learning algorithm enhances the feature discernment of structurally organized boundary voxels, thus establishing the target for contrast optimization within boundary voxels, which is crucial for enhancing the segmentation of boundary regions. Specifically, for voxels xi within boundary regions Bl, learned representations are incentivized to be more congruent with voxel features of the same category while being less congruent with features of disparate categories. Similarly, there are: exp(−d(fi, fj)/τ ) exp(−d(fi, fk)/τ ) Here, fi denotes the representation of the characteristics of xi, d(·, ·) denotes the distance function, and τ represents the temperature coefficient in contrastive learn- while the denominator reflects the similarity between positive and negative instances. Consequently, a higher intraclass similarity coupled with a lower interclass similarity results in a reduced loss. The temperature coefficient is a predefined hyperparameter that governs the model’s sensitivity to negative samples. An increased temperature coefficient τ flattens the distribution of d(fi, fj), leading to a uniform treatment of all negative samples by contrastive loss, which diminishes the model’s learning efficacy. Conversely, setting the temperature coefficient too low directs the model’s focus toward particularly challenging negative samples, which might actually be potential posi- tive samples. This hampers the model’s convergence and degrades its generalization capability. Hence, the determination of the temperature coefficient is crucial. Consistency Module with SAM We use the SAM framework to develop a consistency strategy to mitigate the discrep- ancies that arise from the scanning of different subjects. This strategy comprises three integral components: the volume feature encoder, the 3D cue encoder, and the volume mask decoder. Specifically, the principal contributions can be delineated as follows. Firstly, we introduce an SAM model tailored for the amygdala segmentation task. Unlike existing approaches to improving SAM, the novelty of our model lies in its com- prehensive consideration of factors such as the volume, shape, and relative position of the amygdala. It eschews the conventional slice-by-slice feature extraction in favor of a holistic total volume approach. This method establishes inter-slice associations, rendering it exceptionally suited for structures with intricate core regions. In contrast to traditional algorithms, this model provides a more comprehensive representation of anatomical structures, particularly for smaller entities. Secondly, the incorporation of a 3D prompt encoder ensures the preservation of prior spatial position information and the 3D spatial features provided by the user. In contrast to the 2D prompt encoder, it furnishes more precise relative position cues in a multi-dimensional space. To facilitate the effective amalgamation of the 3D cue information with the embedding features derived from the volume feature encoder, we have devised a volume mask decoder. This decoder harnesses the interaction mechanism between a prior cue embedding and the MRI embedding, facilitating seamless integration. This fusion mechanism adeptly captures nuanced details within the feature space, offering a robust tool for the precise depiction of anatomical structures. SAM constitutes an interactive segmentation methodology predicated on iterative operations, enhancing the model’s representational capacity through the incorporation of positional cues and pre-segmentation data, thereby facilitating continuous optimiza- tion of segmentation accuracy via multiple interactive iterations. In particular, when existing SAM models are applied directly to medical imaging tasks, specifically the amygdala segmentation task, their accuracy is suboptimal. To address this issue, we have meticulously tailored the design of the 2D SAM model for the amygdala seg- mentation context and introduced an advanced SAM model, which comprises three integral encoders for the volume feature, the 3D cue and the volume mask. The function of the Volume Feature Encoder is to transform the input T1w struc- tural image into an MRI embedding vector. Compared to the 2D encoder in the traditional SAM, it allows simultaneous consideration of volume, shape, and relative position, thus offering a more comprehensive representation of anatomical structures and enhancing the processing of complex structures around the core area. To incor- porate interactive cues and existing amygdala segmentation results, a 3D cue encoder is used to encode this prior information. The 3D cue encoder comprises two com- ponents: the click-cue branch and the pre-segmentation cue branch. The click-cue branch converts user-provided cue points into learnable location embedding features to more precisely delineate the location of the amygdala. The pre-segmentation prompt encoder uses the last click of the model during interactive segmentation as input or feeds the pre-segmentation result into the model, thereby prompting the model to adjust and optimize the current segmentation result. To effectively integrate the cue encoder’s information with the feature embedding produced by the feature encoder, and to retrieve and refine the information pertaining to the amygdala, we devise a vol- ume mask detector to ensure the seamless alignment between cue feature information and the original feature encoder output. As the pivotal component, the volume mask decoder is principally designed to exhaustively explore key characteristics within the amygdala, thereby enhancing the model’s efficacy in intricate medical contexts. Its fundamental function is to amalga- mate the MRI embedding vector resulting from the volume feature encoder with the user-provided prompt inputs to ascertain the correlation between them, culminating in precise predictive results. The 3D mask decoder emits MRI embedding and cue embedding through a multi-layered fusion 3D feature encoder, facilitating multi-level and multi-perspective information extraction. This comprehensive fusion mechanism enables the model to assimilate MRI images more profoundly and improves the dis- cernment of medical structures, thus increasing predictive precision. In conclusion, the decoder produces precise predictions of specific structures or lesions within medical images, offering crucial support for clinicians to make accurate diagnoses and devise treatment strategies. We implement an advanced deep-supervised strategy for the formulation of the loss function. Specifically, this involves the integration of supervisory signals in each inter- mediate module and the terminal module, thereby facilitating enhanced gradient propagation and optimizing the efficiency of network training. For each supervisory signal, we incorporate two distinct loss functions, namely, the binary cross-entropy (BCE) loss and the Dice loss. The overall loss function is constructed as follows. To address the issue of imbalance between positive (amygdala) and negative (other tissue) samples in the loss of BCE, an adapted version is employed. For simplicity, the l=0(gi,j = l)logP r(pi,j = l|Ψ) W where l ∈ (0, 1) denotes two categories of labels. The variables pi,j and gi,j represent the predicted and ground truth values, respectively, at the spatial coordinate (i, j) within a CT image characterized by a width of W and a height of H. The symbol Ψ encompasses all parameters of the model and P r(pi,j = l) signifies the predicted probability. The term αi,j ∈ [0, 1] stands for the pixel-specific weight. We define α as where Ai,j denotes the region adjacent to the pixel (i, j), and | · | signifies the calculation of the absolute value. Moreover, the Dice loss is defined as Our algorithm was implemented utilizing the PyTorch framework and utilizes the full capabilities of the Tesla A10 GPU to optimize computational efficiency. AdamW opti- mizer (Xu et al, 2022), a common choice within the transformer architecture domain, was used for routine updates of network parameters. The learning rate was meticu- lously set to 1e − 4 and the weight decay was equally adjusted to 1e − 4 to achieve an equilibrium between the stability of the training and the convergence speed of the model. To mitigate memory consumption, a batch size of 1 was designated, and the training process encompassed 100 iterations. This configuration ensures that the model undergoes sufficient training cycles to attain optimal performance and effectively dis- cern feature representations. During the testing phase, the input image dimensions were reduced to 48 × 64 × 80 to minimize computational demands. Subsequently, no post-processing optimization techniques were employed, thereby ensuring an objective assessment of model performance. In this study, neuroimaging data of human brain scans from three cohorts, includ- ing the Chongqing cohort of the Chinese Color Nest Cohort (CKG-CCNC) (Fan et al, 2023; Liu et al, 2021; Yang et al, 2017), the Beijing Normal University Childhood (Sun et al, 2022) and the Zhejiang University Mul- ticenter Traveler Cohort (ZJU-MTC) (Tong et al, 2020), were used. CKG-CCNC is part of the development component of CCNP (devCCNP), which was collected at Southwest University in Chongqing, China. The devCCNP aims to map normative brain development trajectories within the Chinese population during school years. The devCCNP-CKG dataset included data from 201 participants aged 6-17 years, who were invited to participate in three consecutive waves of data collection at intervals of approximately 1.25 years. At these time points, T1-weighted magnetic resonance imaging (T1w) was performed and images were visually inspected to exclude those with significant head movement artifacts and structural abnormalities. After this ini- tial quality control, the final sample included 427 scans from 198 participants (105 girls and 93 boys). The distribution of participants with one, two and three scans was 48, 71, and 79, respectively, with an average of 2.16 scans per participant. BNU-CAC aims to investigate the impact of childhood adversity on brain development. It consists of 116 subjects (60 girls and 56 boys) aged 8-13 years who have been scanned once per individual and passed quality control protocols. ZJU-MTC includes three adult travelers (Tong et al, 2020) to evaluate image consistency between centers under iden- tical parameters with data collection performed across 10 scanning centers, ensuring comprehensive inter-center comparability. To rigorously assess the model’s performance, standard evaluation metrics for med- ical image segmentation have been incorporated, primarily encompassing similarity metrics and distance-based metrics. These include the Dice coefficient (Milletari et al, 2016) and the Jaccard coefficient (JS), the average surface distance (ASD) and the average symmetric surface distance (ASSD) (Berhane et al, 2020), and the relative volume difference (RVD) (Wang et al, 2008). In addition, precision and recall metrics derived from the confusion matrix are also utilized (Borji et al, 2015). Details of these Dice similarity coefficient (Dice) is used to measure the similarity between 2|VSeg ∩ VGT| |VSeg| + |VGT| where VSeg and VGT represent the voxel sets of the segmentation and GT, respectively. | · | denotes the operation of cardinality computation, which provides the number of elements in a set. Intersection over Union (IoU) is the area of intersection between the predicted segments and the labels divided by the area of union between the predicted segments and the labels. |VSeg ∩ VGT| |VSeg ∪ VGT| Average Surface Distance (ASSD) is a metric based on the surface distance defined as the average distance between the surface voxel S(VSeg) of the segmentation |S(VSeg)| + |S(VGT)| d sVSeg, S(VGT) + d (sVGT , S(VSeg)) where d(s, S(X)) is defined as the minimum Euclidean distance from the voxel s to the surface voxel of the volume X. Relative Volume Difference (RVD) is used to measure the difference between predicted lung infections and GT. We take the absolute value of RVD and report the corresponding results in this work. |VSeg| − |VGT| |VGT| Asymmetry index (AI) assesses the degree of lateralization of the left and right amygdala and is defined as Interclass Correlation Coefficient (ICC) reflects the consistency of results across multiple centers and is defined as (M SB − M SW ) M SB + (k − 1) ∗ M SW M SB denotes the mean square between groups, M SW denotes the mean square within groups, and k is the number of raters. BOLT3D demonstrates unequivocal superiority within the test cohort, attaining exem- plary performance in critical metrics including Dice, JS and HD95 relative to other prevalent algorithms. A comprehensive analysis of these individual metrics and the pronounced performance benefits conferred by BOLT3D on various parameters is presented in Table 1. The Dice coefficient and the JS coefficient serve as imperative metrics for evaluating the extent of overlap in segmentation outcomes. BOLT3D achieves a Dice coefficient of (0.787), and ResU-Net3D (0.841). Analogously, the JS coefficient demonstrates a com- superior segmentation precision. The HD95 and the ASSD quantify the spatial discrep- highlighting the interval between the segmented boundary of the amygdala and the of 1.097 relative to other algorithms such as U-Net3D (1.565), U-Net3D++ (1.963), and ResU-Net3D (1.669). This shows that BOLT3D excels in preserving spatial con- and recall metrics are used to assess false positives. The precision metric quantifies the Table 1: Amygdala index results of 347 scans in the CKG-CCNC cohort proportion of true positive instances among those designated as positive by the model, whereas the recall metric evaluates the proportion of true positive instances correctly identified by the model from all positive instances in the dataset. BOLT3D’s precision and recall are 0.883 and 0.929, respectively, surpassing the performance of other algo- rithms. BOLT3D manifests an admirable balance between these metrics, resulting in an overall superior performance relative to other algorithms. Additionally, BOLT3D achieves a relative RVD of 0.056, significantly lower than that of other algorithms, thereby highlighting the algorithm’s exceptional accuracy in volume prediction. Figure 2 elucidates the comparative segmentation outcomes across multiple algo- rithms on a representative subject, illustrating the superior performance of the proposed algorithm with respect to volumetric and boundary concordance with man- Freesurfer deviates markedly from both the morphological and manual segmentation results. The presence of numerous spaces indicates an inability to accurately capture the structural integrity of the actual amygdala. A closer inspection of its periph- eries reveals that the Freesurfer exhibits numerous irregularities when juxtaposed with the gold standard. The amygdala delineated by Unet exhibits morphological char- acteristics comparable to those of the gold standard. However, an assessment of the superposition effect and edge segmentation results reveals that the volumetric delin- eation by Unet is substantially overestimated. In contrast, our BOLT3D results exhibit a morphological resemblance closer to the gold standard. The superposition effect in BOLT3D is similarly congruent with the gold standard, demonstrating no signifi- cant volumetric overestimation or underestimation, which is corroborated by the edge segmentation results. The superiority of BOLT3D is primarily attributable to the integration of Transformer technology with a boundary contrast learning algorithm, thus improving its ability to capture intricate structural details in the image while\n",
      "\n",
      "Based on this information and on your knowledge, please answer the following question:\n",
      "\n",
      "Please summarize the content of the following article[/INST]Hello!\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recoRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
