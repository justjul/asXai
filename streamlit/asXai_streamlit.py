import os
import streamlit as st
from PIL import Image
import base64
from io import BytesIO
import pandas as pd
from pathlib import Path

BASE_DIR = Path(__file__).parent
img_asxai_logo = BASE_DIR / "img" / "asXai_logo.png"
img_arXiv_monthly = BASE_DIR / 'img'/'arxiv_submissions_monthly.png'
img_scispace_logo = BASE_DIR / 'img'/'sciSpace_logo.png'
img_scite_logo = BASE_DIR / 'img'/'scite_logo.png'
img_paperdoc_logo = BASE_DIR / 'img' / 'paperDoc_logo.jpg'
img_asxai_login = BASE_DIR / 'img' / 'Login.png'

img_pipeline = BASE_DIR / 'img'/'Pipeline_overview.svg'
img_DB_manage = BASE_DIR / 'img'/'DB_management.svg'
img_reranker_train_flow = BASE_DIR / 'img/Reranker_training.svg'

metadata_sample = BASE_DIR / "data"/"metadata_sample_2025.csv"
textdata_sample = BASE_DIR / "data"/"textdata_sample_2025.csv"

img_chat_service = BASE_DIR / 'img'/'chat_service.svg'
img_reranker_model = BASE_DIR / 'img'/'Reranker_model.svg'
img_chunk_embedding = BASE_DIR / 'img'/'Chunk_embedding.svg'
img_triplet_loss = BASE_DIR / 'img'/'Triplet_loss.svg'

img_mlops_stack = BASE_DIR / 'img'/'CICD.png'


@st.cache_data
def load_metadata():
    return pd.read_csv(metadata_sample, index_col=0)


@st.cache_data
def load_textdata():
    return pd.read_csv(textdata_sample, index_col=0)


def get_image_base64(img_path):
    img = Image.open(img_path)
    buffered = BytesIO()
    img.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode()


img_login_base64 = get_image_base64(img_asxai_login)
img_logo_base64 = get_image_base64(img_asxai_logo)


def bilingual(en, fr):
    return en if language == 'En' else fr


# css custom pour les typo / les bocs etc
custom_css = """
<style>
    /* Styles pour sp√©cifier la taille du texte */
    body {
        font-size: 16px; /* Taille de la police pour tout le texte */
        font-family: 'Roboto', sans-serif; /* Utiliser Roboto pour tout le texte */
        background-color: #eee;
    }
    h1 {
        font-size: 40px; /* Taille de la police pour les titres de niveau 1 */
        font-family: 'Roboto', sans-serif; /* Utiliser Roboto pour tout le texte */
    }
    h2 {
        font-size: 28px; /* Taille de la police pour les titres de niveau 2 */
        font-family: 'Roboto Light', sans-serif; /* Utiliser Roboto pour tout le texte */
    }
    p {
        font-size: 16px; /* Taille de la police pour les paragraphes */
        font-family: 'Roboto Light', sans-serif; /* Utiliser Roboto pour tout le texte */
    }

    /* Styles pour les images */
    img {
    display: block;
    margin-left: auto;
    margin-right: auto;
    }

    .img-scale-small {
        width: 200px; /* D√©finir la largeur de l'image */
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    .img-scale-medium {
        width: 400px; /* D√©finir la largeur de l'image */
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    .img-scale-large {
        width: 600px; /* D√©finir la largeur de l'image */
        display: block;
        margin-left: auto;
        margin-right: auto;
    }

    /* Styles pour les blocs */
    .stTabs [data-baseweb="tab-list"] button [data-testid="stMarkdownContainer"] p {
    font-size:20px;
    }

    .container {
    display: flex;
    justify-content: center;
    align-items: center;
    }

    .expander {
    display: flex;
    justify-content: center;
    align-items: center;
    }

    .expander-content {
        font-size: 10px; /* Taille de police pour le contenu de l'expander */
    }

    .stTabs [data-baseweb="tab-list"] {
		gap: 8px;
    }

    .stTabs [data-baseweb="tab"] {
		height: 70px;
        white-space: pre-wrap;
		background-color: #F0F2F6;
		border-radius: 4px 4px 0px 0px;
		gap: 1px;
		padding-top: 10px;
		padding-bottom: 10px;
        padding-left: 20px;
        padding-right: 20px;
        font-size: 10px;
    }

	.stTabs [aria-selected="true"] {
  		background-color: #bf0203;
        color: white;
	}

    .stTabs-content {
        font-size: 10px;
    }

    .streamlit-tabs {
        font-size: 20px;
    }

    div.st-emotion-cache-16txtl3 {
        padding: 2rem 2rem;
    }

    .block-container {
        padding-top: 1rem;
    }



</style>
"""
st.set_page_config(layout="wide")
st.markdown(custom_css, unsafe_allow_html=True)

# DEBUT CODE STREAMLIT************************************************************************************************************

# SOMMAIRE
# st.sidebar.image(str(img_asxai_logo), use_container_width=True)
language = st.sidebar.radio(
    "Language", ["En", "Fr"], horizontal=True, label_visibility="collapsed")

st.sidebar.markdown(f"""
    <a href = "https://goose-beloved-kit.ngrok-free.app/?ngrok-skip-browser-warning=true" target = "_blank" >
        <img src="data:image/png;base64,{img_logo_base64}" width="400" alt="Open asXai Demo">
    </a >
    """, unsafe_allow_html=True)

pages = ["Project overview", "Pipeline overview", "Dataset overview", 'Core components',
         "Performances", "Perspectives", "Conclusions"]
page = st.sidebar.radio("### Layout", pages)
st.sidebar.subheader("Authors")

JF_linkedin_url = "https://www.linkedin.com/in/julien-fournier-63530537"
HN_linkedin_url = "https://www.linkedin.com/in/honorat-nembe-502451242/"
st.sidebar.markdown(f"""
                    - [Julien Fournier]({JF_linkedin_url})
                    - [Honorat Nembe]({HN_linkedin_url})
                    """)


# page 0############################################################################################################################################
if page == pages[0]:
    st.title("A new RAG for science and for all")
    tab1, tab2, tab3 = st.tabs(["Context", "Aims", "Results"])

    with tab1:
        col1, col2, col3 = st.columns([2, 0.2, 2])
        with col1:
            st.header("Retrieval augmented generation on scientific litterature")

            st.markdown(bilingual("""
        Retrieval-Augmented Generation (RAG) pipelines enable large language models to ground their responses in actual documents,
    reducing hallucinations and improving factual accuracy.

    In the scientific domain, this is particularly critical‚Äîresearchers must stay up to date with the latest discoveries
    while relying on accurate, verifiable outputs.
    """, """
        Les pipelines de type **RAG** permettent aux LLMs d‚Äôappuyer leurs r√©ponses sur des documents r√©els, 
    r√©duisant ainsi les hallucinations et am√©liorant la pr√©cision des informations.

    Dans le domaine scientifique, c‚Äôest particuli√®rement crucial : les chercheurs doivent rester √† jour sur les derni√®res d√©couvertes  
    tout en s‚Äôappuyant sur des r√©ponses fiables et v√©rifiables.
    """))

            st.markdown(bilingual("""
    However, most existing RAG-based tools for scientific search and summarization are **commercial**:
        - Behind paywalls
        - Rate-limited
        - Often not transparent or open-source
    """, """
        Cependant, la plupart des outils RAG existants pour la synth√®se scientifique sont **commerciaux** :
        - Derri√®re des paywalls  
        - Limit√©s en nombre de requ√™tes  
        - Souvent ni transparents ni open source
    """))

            st.markdown(bilingual("""
        **The volume of new scientific literature is massive and growing:**

        - üìà **arXiv**: ~24,000 new articles per month
        - üß¨ **bioRxiv**: ~6,000 new articles per month
        - üèõÔ∏è **Semantic Scholar**: >1 million articles indexed yearly
    """, """
        **Le volume de nouvelles publications scientifiques est considerable et en constante augmentation :**

        - üìà **arXiv** : environ 24 000 nouveaux articles par mois  
        - üß¨ **bioRxiv** : environ 6 000 nouveaux articles par mois  
        - üèõÔ∏è **Semantic Scholar** : plus d'un million d'articles index√©s chaque ann√©e
    """))

            st.markdown(bilingual(
                "Some popular commercial tools include:",
                "Exemple de solutions existantes:"
            ))

            col1_1, col1_2, col1_3 = st.columns([1, 1, 1])
            with col1_1:
                st.image(str(img_scispace_logo), width=100, caption="SciSpace")
            with col1_2:
                st.image(str(img_scite_logo), width=100, caption="Scite")
            with col1_3:
                st.image(str(img_paperdoc_logo), width=100, caption="PaperDoc")

        with col3:
            st.header("")
            st.subheader("üìä arXiv Monthly Submissions")
            st.markdown(
                """
                <iframe src="https://tableau.cornell.edu/t/PublicContent/views/arXivSubmissions/LineGraphByArchive?:embed=y&:toolbar=bottom"
                        width="780"
                        height="500"
                        frameborder="0">
                </iframe>
                """,
                unsafe_allow_html=True
            )

    with tab2:
        st.markdown("""
        #### Democratize access to scientific knowledge with a Retrieval-Augmented Generation (RAG) pipeline that is:
        """)

        col1, col2, col3 = st.columns(3)

        with col1:
            st.subheader("üß© Open Source")
            st.markdown("""
            - Transparent codebase and architecture for backend and frontend
            - Built with popular modules (FastAPI, Qdrant, Ollama, React)
            - Enables contributions, auditing, and reproducibility
            - Encourages adoption in research and education
            """)

        with col2:
            st.subheader("üí∏ Free Access")
            st.markdown("""
            - No paywalls or usage limits
            - Hosted or local deployment possible
            - Accessible to individual researchers, educators, and labs
            - Low-resource models supported (e.g., Ollama)
            """)

        with col3:
            st.subheader("üî¨ Built for Scientists")
            st.markdown("""
            - Inline **citations** and reference tracking
            - Capable of **technical Q&A** with proper grounding
            - Interact via **chat interface** enriched by retrieved articles
            - Future: suggests new unexplored research questions
            """)

        st.markdown("---")

    with tab3:
        st.header("üìé Demo of the deployed **asXai RAG pipeline**")

        st.markdown("""
        Submit queries, and receive responses grounded in actual research articles.
        """)

        st.markdown(f"""
            <a href="https://goose-beloved-kit.ngrok-free.app/" target="_blank">
                <img src="data:image/png;base64,{img_login_base64}" width="1400" alt="Open asXai Demo">
            </a>
            """, unsafe_allow_html=True)

        st.info(
            "Note: It may take a few seconds to load the first time depending on server activity.")

# page 1############################################################################################################################################
if page == pages[1]:
    st.title("Pipeline overview")
    tab1, tab2 = st.tabs(
        ["Online Service Architecture", "Offline Service Architecture"])

    with tab1:
        col1, col2, col3, col4 = st.columns([0.3, 1.5, 1.5, 0.3])

        with col2:
            st.header("Backend")
            st.markdown(bilingual("""
            - Three main backend services: **chat, search, retrieval**
            - Communications via **Kafka** and/or using a **shared local file system**.
            - LLMs inferences are served by **Ollama**
            - Semantic retrieval is performed by **Qdrant**
            - A **reranking model** trained on citations and logged in **MLflow**
            """, """
            - Trois services principaux : **chat, search, retrieval**
            - Communication via **Kafka** ou par syst√®me de fichiers local partag√©.
            - Inf√©rences LLM servies par **Ollama**
            - Recherche s√©mantique r√©alis√©e par **Qdrant**
            - Un **mod√®le de reranking** est entra√Æn√© sur les citations et logg√© dans **MLflow**
            """))

        with col3:
            st.header("Frontend and Monitoring")
            st.markdown(bilingual("""
            - React UIX with SSE-based response streaming
            - **Ngrok** + **Nginx** for secure tunneling and local routing.
            - Authentication managed via **Firebase**.
            - **Prometheus**, **Grafana** for monitoring and observability.
            """, """
            - Interface React avec streaming des r√©ponses via SSE
            - **Ngrok** + **Nginx** pour le tunneling s√©curis√© et le routage local.
            - Authentification g√©r√©e via **Firebase**.
            - **Prometheus**, **Grafana** pour le monitoring.
            """))

        col1, col2, col3 = st.columns([0.3, 3, 0.3])
        with col2:
            st.image(str(img_pipeline), caption="System-Level Service Architecture",
                     use_container_width=True)

    with tab2:
        col1, col2, col3, col4 = st.columns([0.2, 1, 1, 0.2])

        with col2:
            st.header("Database management")
            st.markdown(bilingual("""
            - Scientific articles are sourced from **Semantic Scholar** and **arXiv**.
            - PDFs are downloaded via **Selenium**, parsed with **pdfminer**, chunked, and embedded with **e5-large-instruct**.
            - Embeddings are saved in **Qdrant** collection.
            """, """
            - Les articles scientifiques proviennent de **Semantic Scholar** et **arXiv**.
            - Les PDFs sont t√©l√©charg√©s via **Selenium**, pars√©s avec **pdfminer**, d√©coup√©s puis vectoris√©s avec **e5-large-instruct**.
            - Les embeddings sont enregistr√©s dans une collection **Qdrant**.
            """))

        with col3:
            st.header("Reranker model training")
            st.markdown(bilingual("""
            - **Qdrant** provides training data.
            - The model is trained using a **contrastive loss** to align citation relevance.
            - Models are logged and tracked via **MLflow**.
            """, """
            - **Qdrant** fournit les donn√©es d'entra√Ænement.
            - Le mod√®le est entra√Æn√© via une **loss contrastive** pour refl√©ter la pertinence des citations.
            - Les mod√®les sont logg√©s et suivis dans **MLflow**.
            """))

        col1, col2, col3, col4 = st.columns([0.2, 1, 0.51, 0.69])
        with col2:
            st.image(str(img_DB_manage), caption="Database management",
                     use_container_width=True)
        with col3:
            st.image(str(img_reranker_train_flow), caption="Reranker training flow",
                     use_container_width=True)

# page 2############################################################################################################################################
if page == pages[2]:
    st.title("Dataset")

    st.markdown(bilingual("""
    Data used to power the **asXai** RAG pipeline come from Semantics Scholar and arXiv (for now).
    """, """
    Les donn√©es utilis√©es dans le pipeline **asXai** proviennent de Semantic Scholar et arXiv (pour l‚Äôinstant).
    """))

    # Load data
    metadata_df = load_metadata()
    textdata_df = load_textdata()

    metadata_df = metadata_df[['paperId', 'authorName', 'publicationDate',
                               'citationCount', 'fieldsOfStudy', 'openAccessPdf', 'referenceIds']]
    textdata_df = textdata_df[['paperId', 'title',
                               'abstract', 'main_text', 'pdf_status', 'full_text']]

    st.subheader("üîñ Metadata Preview")
    st.markdown(bilingual(f"""
    **metadata** : Titles, authors, publication year, source (e.g. arXiv, Semantic Scholar), citation count, and unique paper ID.
    This metadata is used for filtering, payload enrichment, and user-facing context.

    Fields: **{[col for col in metadata_df.columns]}**
    """, f"""
    **metadata** : Titres, auteurs, ann√©e de publication, source (par ex. arXiv, Semantic Scholar), nombre de citations et ID unique de l‚Äôarticle.
    Ces m√©tadonn√©es servent au filtrage, √† l‚Äôenrichissement des payloads et au contexte utilisateur.

    Champs : **{[col for col in metadata_df.columns]}**
    """))
    st.dataframe(metadata_df, use_container_width=True)

    st.subheader("üìÑ Text Data Preview")
    st.markdown(bilingual(f"""
    **textdata**: holds text extracted from the article PDFs. These are chunked, embedded and stored to Qdrant
    for retrieval.

    Fields: **{[col for col in textdata_df.columns]}**
    """, f"""
    **textdata** : contient le texte extrait des fichiers PDF des articles. Il est d√©coup√© en segments, vectoris√© et stock√© dans Qdrant
    pour √™tre utilis√© lors de la r√©cup√©ration d‚Äôinformations.

    Champs : **{[col for col in textdata_df.columns]}**
    """))
    st.dataframe(textdata_df, use_container_width=True)


# page 3############################################################################################################################################
if page == pages[3]:
    st.title("Core Components")

    tab1, tab2, tab3, tab4 = st.tabs(
        ["Chunk Embedding", "Model Training", "Chat Service", "MLOps stack"])

    # Tab 1: Chunk Embedding
    with tab1:
        col1, col2 = st.columns([0.6, 1.2])

        with col1:
            st.subheader("Embedding Scientific Texts into Semantic Vectors")
            st.markdown(bilingual(
                """
            - Articles are split into overlapping windows of **512 tokens**.
            - A central chunk of **~170 tokens** is extracted and used for embedding.
            - Embeddings are computed using a pre-trained model (**e5-large-instruct**) and stored in **Qdrant** with article metadata.
            - This strategy balances **granularity** and **context**, ensuring effective semantic search across long documents.
            """,
                """
            - Les articles sont divis√©s en fen√™tres superpos√©es de **512 tokens**.
            - Un segment central d‚Äôenviron **170 tokens** est extrait et utilis√© pour l'embedding.
            - Les embeddings sont calcul√©s √† l‚Äôaide d‚Äôun mod√®le pr√©-entra√Æn√© (**e5-large-instruct**) et stock√©s dans **Qdrant** avec les m√©tadonn√©es.
            - Cette strat√©gie √©quilibre **granularit√©** et **contexte**, garantissant une recherche s√©mantique efficace dans les documents longs.
            """))

        with col2:
            st.subheader("")
            st.subheader("")
            st.image(str(img_chunk_embedding), use_container_width=True)

    # Tab 2: Triplet Loss Model Training
    with tab2:
        col1, col2, col3 = st.columns([0.8, 1, 0.3])
        with col1:
            st.subheader("Reranker Training with Triplet Loss")
            st.markdown(bilingual(
                """
                - A **transformer-based reranker** is trained to score and rank retrieved chunks.
                - Training uses a **triplet loss** with:
                    - Anchor: article embeddings
                    - Positive: cited article embeddings
                    - Negative: non-cited article embeddings
                - Optimizes the embedding space to prioritize **citation alignment**.
                """,
                """
                - Un **reranker bas√© sur un transformeur** est entra√Æn√© pour scorer et classer les segments retrouv√©s.
                - L'entra√Ænement utilise une **perte triplet** avec :
                    - Anchor : embeddings de l'article de d√©part
                    - Positive : embeddings d‚Äôun article cit√©
                    - Negative : embeddings d‚Äôun article non cit√©
                - L‚Äôespace d‚Äôembedding est optimis√© pour favoriser l‚Äô**alignement des citations**.
                """))
        with col2:
            st.subheader("")
            st.image(str(img_reranker_model), use_container_width=True,
                     caption="8-layer reranker transformer")

        col1, col2, col3 = st.columns([0.8, 0.7, 0.6])
        with col2:
            st.image(str(img_triplet_loss), use_container_width=True)

    # Tab 3: Chat Service
    with tab3:
        col1, col2 = st.columns([0.6, 1.2])

        with col1:
            st.subheader("Interactive Chat Grounded in Literature")
            st.markdown(bilingual(
                """
                - User's query is processed according to the **initial instruction** and reformulated by the **Chat worker**
                which send it to the **Search API**
                - Search queries are parsed with an LLM (e.g., **gemma3:12b** via **Ollama**).
                - The top-K retrieved chunks are returned to the Chat service and added to the context of the LLM for generation
                according to the **Refinement instruction**.
                - The response includes **article citations** and feeds a **Citation Score**, used to train a lightweight
                rescorer model.
                - All responses are streamed back to the frontend using **Server-Sent Events (SSE)**.
                """,
                """
                - La requ√™te de l‚Äôutilisateur est trait√©e via une **instruction initiale** puis reformul√©e par le **Chat worker** avant d‚Äô√™tre envoy√©e √† l‚Äô**API de recherche**.
                - Les requ√™tes sont analys√©es avec un LLM (par exemple, **gemma3:12b** via **Ollama**).
                - Les top-K segments retrouv√©s sont ajout√©s au contexte de g√©n√©ration du LLM selon une **instruction de raffinement**.
                - Les r√©ponses incluent les **citations** utilis√©es et alimentent un **score de citation** pour entra√Æner un reranker l√©ger.
                - Toutes les r√©ponses sont transmises en direct via **Server-Sent Events (SSE)**.
                """))

        with col2:
            st.subheader("")
            st.image(str(img_chat_service), use_container_width=True)

        col1, col2 = st.columns([1, 1])
        with col1:
            st.markdown(""" **Initial instruction**: \n
            If you can answer the user's question accurately based on your knowledge
            and previous messages in the conversation: answer it direclty and
            if you used previous messages, cite your source by including (id)
            where id are the ARTICLE IDs from the database but DO NOT include
            external references. \n
            If your answer needs to be cross-checked for accuracy:
            Step 1: Define at least three questions that covers different aspects
            of the user's query.
            Step 2: Expand each of these questions by rephrasing them in a more
            descriptive and complete sentence,
            suitable for retrieving relevant scientific documents.
            Make complete sentences not just key words.
            Step 3: At the end, return the questions defined at step 2, as: \n
              'SEARCHING: question1' 'SEARCHING: question2' etc.

            """)

        with col2:
            st.markdown("""**Refinement instruction**:\n
            Provide an accurate answer to the user's question based on the
            provided articles and on your knowledge.\n
            Structure your response in multiple paragraph describing one
            element of the response.
            Cite your sources by including where appropriate (ARTICLE_ID) where
            ARTICLE_ID are the hashed ids provided in the context\n
            for each article.\n

            """)

    with tab4:
        st.subheader("DevOps, MLOps & Monitoring")

        st.markdown(bilingual(
            """
            This system follows basic **CI/CD** and **MLOps** practices for production-level pipelines.
            """,
            """
            Ce syst√®me suit les bonnes pratiques de base de **CI/CD** et de **MLOps** pour les pipelines de niveau production.
            """))

        col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
        with col1:
            st.markdown(bilingual(
                """
                #### üîÑ Continuous Integration / Continuous Deployment (CI/CD)
                - GitHub handles code versioning.
                - Docker + Docker Compose define infrastructure as code for all services.
                - Local and remote environments mirror each other for reproducibility.
                """,
                """
                #### üîÑ Int√©gration et d√©ploiement continus (CI/CD)
                - GitHub g√®re la gestion de version du code.
                - Docker + Docker Compose d√©finissent l‚Äôinfrastructure comme du code pour tous les services.
                - Les environnements locaux et distants sont identiques pour garantir la reproductibilit√©.
                """))
        with col2:
            st.markdown(bilingual(
                """
                #### üß™ MLOps with MLflow
                - Reranker models are logged, versioned, and evaluated in **MLflow**.
                - Best-performing model is promoted to "production" and automatically used by workers.
                """,
                """
                #### üß™ MLOps avec MLflow
                - Les mod√®les de reranking sont enregistr√©s, versionn√©s et √©valu√©s avec **MLflow**.
                - Le meilleur mod√®le est promu en production et utilis√© automatiquement.
                """))
        with col3:
            st.markdown(bilingual(
                """
                #### üìä Monitoring
                - **Prometheus** tracks API call volume, latency, and system resource usage.
                - **Grafana** dashboards provide real-time observability.
                - Alerts can be configured for system failures or overload conditions.
                """,
                """
                #### üìä Monitoring
                - **Prometheus** suit les volumes d‚Äôappels API, latence et consommation syst√®me.
                - Les tableaux de bord **Grafana** fournissent une observabilit√© en temps r√©el.
                - Des alertes peuvent √™tre configur√©es en cas de panne ou de surcharge.
                """))
        with col4:
            st.markdown(bilingual(
                """
                #### üê≥ Containerized Ecosystem
                - Every service (LLM, Qdrant, frontend, Chat service, Kafka, Search service, etc.) is deployed in its own container.
                - Docker Compose ensures they communicate correctly and restart on failure.
                """,
                """
                #### üê≥ √âcosyst√®me Conteneuris√©
                - Chaque service (LLM, Qdrant, frontend, Chat, Kafka, Search, etc.) est d√©ploy√© dans son propre conteneur.
                - Docker Compose garantit leur bon fonctionnement et red√©marrage en cas d‚Äô√©chec.
                """))

        st.markdown(bilingual(
            """
            > ‚úÖ This setup enables fast iteration, safe deployment, and continuous learning from usage data.
            """,
            """
            > ‚úÖ Cette configuration permet une it√©ration rapide, un d√©ploiement s√©curis√©, et un apprentissage continu √† partir des donn√©es d‚Äôusage.
            """))

        col1, col2, col3 = st.columns([0.3, 1, 0.3])
        with col2:
            st.image(str(img_mlops_stack), use_container_width=True)


# page 4############################################################################################################################################
if page == pages[4]:
    st.title("Performance Overview")

    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        st.markdown(bilingual(
            """
            ### ‚öôÔ∏è System Metrics (from Prometheus)

            - üîÑ Average search latency: **1.2s**
            - üì• Chat inference latency: **~2.8s** per query
            - üß† Ollama inference time: **~1.5s**
            - ‚úÖ Uptime stability: **100%** (Docker auto-restart on failure)
            """,
            """
            ### ‚öôÔ∏è Indicateurs syst√®me (via Prometheus)

            - üîÑ Latence moyenne des recherches : **1.2s**
            - üì• Latence d'inf√©rence du chat : **~2.8s** par requ√™te
            - üß† Temps d'inf√©rence Ollama : **~1.5s**
            - ‚úÖ Stabilit√© de disponibilit√© : **100%** (reboot auto via Docker)
            """))
    with col2:
        st.markdown(bilingual(
            """
            ### üìä Model Performance (MLflow)

            - Latest Reranker: `v2`
            - Accuracy: **81.2%**
            - Triplet loss: **0.0018**
            - Promoted to production: **2025-06-06**
            """,
            """
            ### üìä Performance du mod√®le (MLflow)

            - Reranker actuel : `v2`
            - Pr√©cision : **81.2%**
            - Perte triplet : **0.0018**
            - Promu en production : **2025-06-06**
            """))

    with col3:
        st.markdown(bilingual(
            """
            ### ‚úÖ Observed Improvements

            - With reranking, relevant citations appear in top 3 results **+37%** more often.
            - Latency with reranking pipeline remains < 3.5s end-to-end.
            """,
            """
            ### ‚úÖ Am√©liorations observ√©es

            - Gr√¢ce au reranking, les citations pertinentes apparaissent dans le top 3 **+37%** plus souvent.
            - La latence totale reste < 3.5s avec reranking.
            """))

# page 5############################################################################################################################################
if page == pages[5]:
    st.title("Future improvements")

    st.markdown(bilingual(
        """
        The current pipeline is functional and modular, but there are several areas planned for improvement:
        """,
        """
        Le pipeline actuel est fonctionnel et modulaire, mais plusieurs pistes d‚Äôam√©lioration sont pr√©vues :
        """))
    col1, col2, col3 = st.columns([1, 1, 1])

    with col1:
        st.markdown(bilingual(
            """
            #### üß† Model & Retrieval Enhancements
            - Improve fine-tuning of the notebook specific blender models.
            - Extract citations from pdf to increase training set of the Reranker model.
            """,
            """
            #### üß† Am√©liorations du mod√®le et de la recherche
            - Mieux ajuster les mod√®les blender sp√©cifiques aux notebooks.
            - Extraire les citations des PDFs pour enrichir l'entra√Ænement du reranker.
            """))
    with col2:
        st.markdown(bilingual(
            """
            #### ‚öôÔ∏è System Optimization
            - Implement multiple LLMs agents working in parallel to generate different parts of the response.
            - Go from mid-size to large-scale LLMs
            """,
            """
            #### ‚öôÔ∏è Optimisation syst√®me
            - Mettre en place des agents LLM parall√®les pour g√©n√©rer diff√©rentes parties des r√©ponses.
            - Passer √† des mod√®les LLM plus grands.
            """))
    with col3:
        st.markdown(bilingual(
            """
            #### üß™ Evaluation & Feedback
            - Build internal evaluation set with human-labeled relevance judgments.
            - Add user feedback mechanism to improve reranking in-the-loop.
            """,
            """
            #### üß™ √âvaluation & retour utilisateur
            - Cr√©er un jeu d‚Äô√©valuation avec annotations humaines sur la pertinence.
            - Ajouter un retour utilisateur pour affiner le reranking en continu.
            """))

    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        st.markdown(bilingual(
            """
            #### üåê Deployment & Scalability
            - Migrate to Kubernetes for better orchestration and autoscaling.
            - Replace Ngrok with Cloudflare Tunnel or custom domain.
            """,
            """
            #### üåê D√©ploiement & mont√©e en charge
            - Migrer vers Kubernetes pour une meilleure orchestration et mont√©e en charge.
            - Remplacer Ngrok par Cloudflare Tunnel ou un nom de domaine d√©di√©.
            """))
    with col2:
        st.markdown(bilingual(
            """
            #### üßæ UI/UX Improvements
            - Display citation scores and confidence alongside chat answers.
            - Add option to export citations to a new notebook or bibtex file
            - Make citation suggestions on texts provided by the user
            """,
            """
            #### üßæ Am√©liorations de l‚ÄôUI/UX
            - Afficher le score de citation et la confiance avec chaque r√©ponse.
            - Permettre d‚Äôexporter les citations vers un notebook ou fichier BibTeX.
            - Proposer des suggestions de citations pour des textes fournis.
            """))

    st.markdown(bilingual(
        """
        > ‚úÖ These improvements should make asXai faster, more accurate, and more valuable to scientific users.
        """,
        """
        > ‚úÖ Ces am√©liorations rendront asXai plus rapide, pr√©cis et utile pour les chercheurs.
        """))


# page 6############################################################################################################################################
if page == pages[6]:
    st.title("Conclusion")

    st.markdown(bilingual(
        """
        **asXai** is a modular and functional **Retrieval-Augmented Generation (RAG)** pipeline
        built for scientific literature.
        """,
        """
        **asXai** est un pipeline **RAG (Retrieval-Augmented Generation)** modulaire et fonctionnel,
        con√ßu pour la litt√©rature scientifique.
        """))

    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        st.markdown(bilingual(
            """
            ### üîç What We've Built
            - A working chat assistant grounded in real, up-to-date scientific publications.
            - Fully containerized system ready to deploy.
            - Custom training of a reranker using triplet loss, monitored through MLflow.
            - A React-based frontend with Firebase Auth and SSE-based live response streaming.
            """,
            """
            ### üîç Ce que nous avons construit
            - Un assistant de chat fonctionnel ancr√© dans des publications scientifiques r√©centes et r√©elles.
            - Un syst√®me enti√®rement conteneuris√©, pr√™t √† d√©ployer.
            - Entra√Ænement personnalis√© du reranker via une perte triplet, suivi avec MLflow.
            - Un frontend React avec authentification Firebase et flux SSE.
            """))
    with col2:
        st.markdown(bilingual(
            """
            ### üß© What Makes It Different
            - **Open-source**, **free**, and built for researchers and students.
            - Transparent and adaptable for research, education, or production use.
            - Citation-aware responses and technical fluency using lightweight LLMs.
            """,
            """
            ### üß© Ce qui le rend unique
            - **Open-source**, **gratuit**, con√ßu pour chercheurs et √©tudiants.
            - Transparent et adaptable pour la recherche, l‚Äôenseignement ou la production.
            - R√©ponses sensibles aux citations et techniquement pr√©cises gr√¢ce √† des LLMs l√©gers.
            """))
    with col3:
        st.markdown(bilingual(
            """
            ### üî≠ What‚Äôs Next
            - Continue improving model quality, speed, and user experience.
            - Enable user feedback loops for adaptive reranking.
            - Deploy a public-facing, persistent version for the research community.
            - Improve monitoring.
            """,
            """
            ### üî≠ Prochaines √©tapes
            - Continuer √† am√©liorer la qualit√© du mod√®le, la vitesse et l‚Äôexp√©rience utilisateur.
            - Int√©grer un retour utilisateur pour affiner le reranking.
            - D√©ployer une version publique et persistante pour la communaut√© scientifique.
            - Am√©liorer la supervision.
            """))

    st.divider()

    # Final demo section
    st.subheader(bilingual("üöÄ Try the Demo", "üöÄ Lancer la d√©mo"))
    demo_url = "https://goose-beloved-kit.ngrok-free.app/?ngrok-skip-browser-warning=true"
    st.markdown(bilingual(
        f"""Click to open the live [asXai demo]({demo_url}) in a new tab.""",
        f"""Cliquez pour ouvrir la [d√©mo asXai]({demo_url}) dans un nouvel onglet.""",
    ))
